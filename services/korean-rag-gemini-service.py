#!/usr/bin/env python3
"""
Korean RAG Gemini Response Service - Gemini APIë¥¼ í™œìš©í•œ í•œêµ­ì–´ RAG ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤

ğŸ”„ MIGRATION TO LOCAL LLMs PLANNED ğŸ”„
í–¥í›„ Local LLM ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜ˆì • - ì´ íŒŒì¼ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½ë  ì˜ˆì •ì…ë‹ˆë‹¤:

1. Gemini API ëŒ€ì²´ ëŒ€ìƒ ëª¨ë¸ë“¤:
   - GPT-OSS (Local GPT variants)
   - DeepSeek-R1 (ë¡œì»¬ ì¶”ë¡  ëª¨ë¸)
   - Gemma3 (Googleì˜ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸)
   - Qwen3 (Alibabaì˜ ë‹¤êµ­ì–´ ëª¨ë¸)
   - Ollama (ë¡œì»¬ LLM ì‹¤í–‰ í”Œë«í¼)

2. ì£¼ìš” ë³€ê²½ ì˜ì—­:
   - import google.generativeai -> ollama, transformers, vllm ë“±ìœ¼ë¡œ êµì²´
   - GeminiRAGService -> LocalLLMServiceë¡œ í´ë˜ìŠ¤ëª… ë³€ê²½
   - API í‚¤ ê¸°ë°˜ ì¸ì¦ -> ë¡œì»¬ ëª¨ë¸ ë¡œë”© ë°©ì‹ìœ¼ë¡œ ë³€ê²½
   - í´ë¼ìš°ë“œ API í˜¸ì¶œ -> ë¡œì»¬ ì¶”ë¡  í˜¸ì¶œë¡œ ë³€ê²½

3. ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­:
   - GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìµœì í™” í•„ìš”
   - ë°°ì¹˜ ì²˜ë¦¬ ë° ë™ì‹œì„± ì œì–´ êµ¬í˜„
   - ëª¨ë¸ ë¡œë”© ì‹œê°„ ìµœì í™” (ìºì‹±, warm-up)
   - í•œêµ­ì–´ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë° íŒŒì¸íŠœë‹ ê³ ë ¤

4. ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­:
   - CUDA/ROCm GPU ì§€ì› ì„œë²„
   - ì¶©ë¶„í•œ VRAM (16GB+ ê¶Œì¥)
   - ëª¨ë¸ íŒŒì¼ ì €ì¥ì†Œ (100GB+ SSD)
   - ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Docker/Kubernetes)
"""

import os
import asyncio
import logging
import json
from datetime import datetime
from typing import List, Dict, Any, Optional, Union

# FastAPI ë° ê¸°ë³¸ ì˜ì¡´ì„±
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import uvicorn

# ğŸ”„ LOCAL LLM MIGRATION POINT 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì„¹ì…˜
# í˜„ì¬: Gemini API ì‚¬ìš©
# í–¥í›„: ì•„ë˜ì™€ ê°™ì´ ë¡œì»¬ LLM ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ êµì²´ ì˜ˆì •
# 
# import ollama                    # Ollama ë¡œì»¬ LLM í”Œë«í¼
# import torch                     # PyTorch for model inference
# from transformers import (       # HuggingFace Transformers
#     AutoTokenizer, 
#     AutoModelForCausalLM,
#     pipeline
# )
# import vllm                      # vLLM for optimized inference
# from openai import OpenAI        # OpenAI-compatible local endpoints
#
# LOCAL_LLM_CONFIG = {
#     "ollama": {"host": "localhost", "port": 11434},
#     "vllm": {"host": "localhost", "port": 8000},
#     "model_path": "/models/",  # ë¡œì»¬ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
#     "available_models": [
#         "llama2-13b-korean",
#         "gemma-7b-ko", 
#         "qwen-14b-chat",
#         "deepseek-coder-33b"
#     ]
# }
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸ google-generativeai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-generativeai' ì‹¤í–‰ í•„ìš”")

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# Pydantic ëª¨ë¸
class KoreanAnalysis(BaseModel):
    original_query: str
    processed_query: str
    tokenized: List[str]
    keywords: List[str]

class GenerateRequest(BaseModel):
    query: str
    context: str = ""
    korean_analysis: Optional[Union[KoreanAnalysis, Dict[str, Any]]] = None
    user_id: Optional[str] = "default"

class GenerateResponse(BaseModel):
    response: str
    model_used: str
    processing_time: float
    context_length: int
    korean_optimized: bool

class HealthResponse(BaseModel):
    status: str
    gemini_api_available: bool
    model: Optional[str]
    timestamp: str

# ğŸ”„ LOCAL LLM MIGRATION POINT 2: ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ ì •ì˜
# í˜„ì¬: GeminiRAGService - Gemini API ê¸°ë°˜ ì„œë¹„ìŠ¤
# í–¥í›„: LocalLLMServiceë¡œ í´ë˜ìŠ¤ëª… ë³€ê²½ ë° êµ¬ì¡° ê°œí¸
#
# class LocalLLMService:
#     def __init__(self, model_type="ollama"):
#         self.model_type = model_type  # ollama, vllm, transformers
#         self.model_name = None
#         self.model = None
#         self.tokenizer = None
#         self.client = None  # HTTP client for local API
#         self.gpu_available = torch.cuda.is_available()
#         self.model_config = LOCAL_LLM_CONFIG[model_type]
#
class GeminiRAGService:
    def __init__(self):
        self.model = None
        self.model_name = "gemini-1.5-flash"
        self.api_key = None
        self.initialized = False
        
    def initialize(self):
        """ğŸ”„ LOCAL LLM MIGRATION POINT 3: ëª¨ë¸ ì´ˆê¸°í™” ë©”ì†Œë“œ
        í˜„ì¬: Gemini API í‚¤ ê¸°ë°˜ ì´ˆê¸°í™”
        í–¥í›„: ë¡œì»¬ ëª¨ë¸ ë¡œë”© ë°©ì‹ìœ¼ë¡œ ë³€ê²½
        
        # ë¡œì»¬ LLM ì´ˆê¸°í™” ì˜ˆì‹œ:
        # def initialize_local_llm(self, model_name="gemma-7b-ko"):
        #     if self.model_type == "ollama":
        #         self.client = ollama.Client(host=self.model_config["host"])
        #         # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìë™ ë‹¤ìš´ë¡œë“œ
        #         try:
        #             self.client.pull(model_name)
        #         except Exception as e:
        #             logger.warning(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        #             
        #     elif self.model_type == "transformers":
        #         self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        #         self.model = AutoModelForCausalLM.from_pretrained(
        #             model_name,
        #             torch_dtype=torch.float16,
        #             device_map="auto" if self.gpu_available else "cpu"
        #         )
        #         
        #     elif self.model_type == "vllm":
        #         # vLLM ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë¼ê³  ê°€ì •
        #         self.client = OpenAI(
        #             base_url=f"http://{self.model_config['host']}:{self.model_config['port']}/v1",
        #             api_key="token"  # vLLMì—ì„œëŠ” ì„ì˜ í† í°
        #         )
        """
        if not GEMINI_AVAILABLE:
            logger.error("âŒ Gemini API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
            
        # API í‚¤ í™•ì¸ (í–¥í›„ ì œê±° ì˜ˆì • - ë¡œì»¬ ëª¨ë¸ì—ì„œëŠ” ë¶ˆí•„ìš”)
        self.api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            logger.error("âŒ GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            logger.info("ğŸ’¡ export GEMINI_API_KEY=your_api_key_here")
            return False
        
        try:
            # Gemini API ì„¤ì • (í–¥í›„ ë¡œì»¬ ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ êµì²´)
            genai.configure(api_key=self.api_key)
            
            # ëª¨ë¸ ì´ˆê¸°í™” (í–¥í›„ ë¡œì»¬ ëª¨ë¸ ê°ì²´ë¡œ êµì²´)
            self.model = genai.GenerativeModel(
                model_name=self.model_name,
                generation_config={
                    "temperature": 0.3,  # ë¡œì»¬ ëª¨ë¸ì—ì„œë„ ë™ì¼í•œ ì„¤ì • ìœ ì§€
                    "top_p": 0.8,
                    "top_k": 40,
                    "max_output_tokens": 2048,  # max_new_tokensë¡œ ë³€ê²½ ì˜ˆì •
                }
            )
            
            logger.info(f"âœ… Gemini API ì´ˆê¸°í™” ì™„ë£Œ: {self.model_name}")
            self.initialized = True
            return True
            
        except Exception as e:
            logger.error(f"âŒ Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def create_korean_prompt(self, query: str, context: str, korean_analysis: Optional[KoreanAnalysis] = None) -> str:
        """í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ê¸°ë³¸ í•œêµ­ì–´ RAG í”„ë¡¬í”„íŠ¸
        base_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

**ë‹µë³€ ì§€ì¹¨:**
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. êµ¬ì²´ì ì¸ ì •ë³´ì™€ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”
5. ë§Œì•½ ë¬¸ì„œ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ë©´ ê·¸ë ‡ê²Œ ì•Œë ¤ì£¼ì„¸ìš”

**ì‚¬ìš©ì ì§ˆë¬¸:** {query}

**ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:**
{context}

**ë‹µë³€:**"""

        # í•œêµ­ì–´ ë¶„ì„ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if korean_analysis:
            korean_info = f"""
**í•œêµ­ì–´ ë¶„ì„ ì •ë³´:**
- ì²˜ë¦¬ëœ ì¿¼ë¦¬: {korean_analysis.processed_query}
- ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(korean_analysis.keywords)}
- í† í°: {', '.join(korean_analysis.tokenized)}

"""
            # í”„ë¡¬í”„íŠ¸ì— í•œêµ­ì–´ ë¶„ì„ ì •ë³´ ì‚½ì…
            base_prompt = base_prompt.replace("**ì‚¬ìš©ì ì§ˆë¬¸:**", korean_info + "**ì‚¬ìš©ì ì§ˆë¬¸:**")
        
        return base_prompt
    
    async def generate_response(self, request: GenerateRequest) -> GenerateResponse:
        """ğŸ”„ LOCAL LLM MIGRATION POINT 4: ì‘ë‹µ ìƒì„± ë©”ì†Œë“œ
        í˜„ì¬: Gemini API í˜¸ì¶œ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        í–¥í›„: ë¡œì»¬ LLM ì¶”ë¡  í˜¸ì¶œë¡œ ë³€ê²½
        
        # ë¡œì»¬ LLM ì‘ë‹µ ìƒì„± ì˜ˆì‹œ:
        # async def generate_local_response(self, request: GenerateRequest):
        #     if self.model_type == "ollama":
        #         response = await asyncio.to_thread(
        #             self.client.chat,
        #             model=self.model_name,
        #             messages=[{"role": "user", "content": prompt}],
        #             options={"temperature": 0.3, "top_p": 0.8}
        #         )
        #         return response['message']['content']
        #         
        #     elif self.model_type == "transformers":
        #         inputs = self.tokenizer(prompt, return_tensors="pt")
        #         with torch.no_grad():
        #             outputs = self.model.generate(
        #                 **inputs,
        #                 max_new_tokens=2048,
        #                 temperature=0.3,
        #                 top_p=0.8,
        #                 do_sample=True,
        #                 pad_token_id=self.tokenizer.eos_token_id
        #             )
        #         return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        #         
        #     elif self.model_type == "vllm":
        #         response = await self.client.chat.completions.create(
        #             model=self.model_name,
        #             messages=[{"role": "user", "content": prompt}],
        #             temperature=0.3,
        #             top_p=0.8,
        #             max_tokens=2048
        #         )
        #         return response.choices[0].message.content
        """
        start_time = datetime.now()
        
        # API í‚¤ê°€ ì—†ëŠ” ê²½ìš° fallback ì‘ë‹µ ì œê³µ
        if not self.initialized:
            logger.warning("Gemini APIê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Fallback ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.")
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
            fallback_response = f"""ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

**ì§ˆë¬¸:** {request.query}

**ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:**
{request.context[:500]}{'...' if len(request.context) > 500 else ''}

**ë‹µë³€:** 
ë¬¸ì„œ ë‚´ìš©ì„ ê²€í† í•œ ê²°ê³¼, í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ AI ì‘ë‹µì„ ìœ„í•´ì„œëŠ” Gemini API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

âš ï¸ í˜„ì¬ Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì œí•œëœ ì‘ë‹µì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì™„ì „í•œ AI ì‘ë‹µì„ ìœ„í•´ `export GEMINI_API_KEY=your_api_key` ëª…ë ¹ìœ¼ë¡œ API í‚¤ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”."""

            processing_time = (datetime.now() - start_time).total_seconds()
            
            return GenerateResponse(
                response=fallback_response,
                model_used="fallback-mode",
                processing_time=processing_time,
                context_length=len(request.context),
                korean_optimized=False
            )
        
        try:
            # í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.create_korean_prompt(
                query=request.query,
                context=request.context,
                korean_analysis=request.korean_analysis
            )
            
            # Gemini API í˜¸ì¶œ
            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if response.candidates and len(response.candidates) > 0:
                generated_text = response.candidates[0].content.parts[0].text.strip()
            else:
                generated_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return GenerateResponse(
                response=generated_text,
                model_used=self.model_name,
                processing_time=processing_time,
                context_length=len(request.context),
                korean_optimized=True
            )
            
        except Exception as e:
            logger.error(f"Gemini API ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ì „ì—­ Gemini ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiRAGService()

# FastAPI ì•± ì„¤ì •
app = FastAPI(
    title="Korean RAG Gemini Service",
    description="Gemini APIë¥¼ í™œìš©í•œ í•œêµ­ì–´ RAG ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤",
    version="1.0.0-gemini"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ Gemini API ì´ˆê¸°í™”"""
    logger.info("ğŸš€ Korean RAG Gemini Service ì‹œì‘ ì¤‘...")
    success = gemini_service.initialize()
    if success:
        logger.info("âœ… Gemini API ì—°ê²° ë° ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨ - ëª¨ì˜ ëª¨ë“œë¡œ ì‹¤í–‰")

# API ì—”ë“œí¬ì¸íŠ¸
@app.get("/", response_model=dict)
async def root():
    return {
        "service": "Korean RAG Gemini Service",
        "version": "1.0.0-gemini",
        "status": "running",
        "description": "Gemini APIë¥¼ í™œìš©í•œ í•œêµ­ì–´ RAG ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤",
        "features": [
            "Gemini 1.5 Flash ëª¨ë¸",
            "í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸",
            "ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±",
            "í•œêµ­ì–´ ë¶„ì„ ì •ë³´ í™œìš©"
        ]
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    return HealthResponse(
        status="healthy" if gemini_service.initialized else "degraded",
        gemini_api_available=gemini_service.initialized,
        model=gemini_service.model_name if gemini_service.initialized else None,
        timestamp=datetime.now().isoformat()
    )

@app.post("/generate", response_model=GenerateResponse)
async def generate_response(request: GenerateRequest):
    """
    ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ì‘ë‹µ ìƒì„±
    """
    try:
        # ì…ë ¥ ê²€ì¦
        if not request.query.strip():
            raise HTTPException(status_code=400, detail="ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # ì»¨í…ìŠ¤íŠ¸ëŠ” ë¹ˆ ë¬¸ìì—´ë„ í—ˆìš© (ì¼ë°˜ ì§ˆì˜ì‘ë‹µ ëª¨ë“œ)
        
        # Gemini APIë¡œ ì‘ë‹µ ìƒì„±
        response = await gemini_service.generate_response(request)
        
        logger.info(f"âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ - ì‚¬ìš©ì: {request.user_id}, ì²˜ë¦¬ì‹œê°„: {response.processing_time:.3f}s")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì‘ë‹µ ìƒì„± API ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def get_stats():
    """ì„œë¹„ìŠ¤ í†µê³„ ì •ë³´"""
    return {
        "service": "Korean RAG Gemini Service",
        "gemini": {
            "available": gemini_service.initialized,
            "model": gemini_service.model_name,
            "api_key_configured": bool(gemini_service.api_key)
        },
        "features": {
            "korean_optimization": True,
            "context_based_generation": True,
            "multilingual": True
        },
        "version": "1.0.0-gemini"
    }

if __name__ == "__main__":
    print("ğŸš€ Korean RAG Gemini Service ì‹œì‘ ì¤‘...")
    print("ğŸ“ Gemini API ê¸°ë°˜ í•œêµ­ì–´ ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤")
    print("ğŸ”— Running on http://0.0.0.0:8009")
    print("âœ… ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±, í•œêµ­ì–´ ìµœì í™”")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8009,
        log_level="info"
    )