#!/usr/bin/env python3
"""
Korean RAG Service - TF-IDF ê¸°ë°˜ í•œêµ­ì–´ íŠ¹í™” RAG API
PyTorch/Transformers ì—†ì´ ì‹¤ì œ í•œêµ­ì–´ RAG ê¸°ëŠ¥ ì œê³µ
"""

import sys
import os
import asyncio
import logging
import json
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

# FastAPI ë° ê¸°ë³¸ ì˜ì¡´ì„±
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import uvicorn
import httpx

# í•œêµ­ì–´ ì²˜ë¦¬ ë° ë²¡í„°í™” (ê¸°ì¡´ backend ëª¨ë“ˆ í™œìš©)
sys.path.append('/home/qportal-dev/ë°”íƒ•í™”ë©´/sdc_i/backend')
from services.korean_embeddings import KoreanEmbeddingService, KoreanTextProcessor

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# FastAPI ì•± ì„¤ì •
app = FastAPI(
    title="Korean RAG Service",
    description="í•œêµ­ì–´ íŠ¹í™” RAG API ì„œë¹„ìŠ¤ (TF-IDF + Kiwi)",
    version="2.0.0-korean-optimized"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
korean_embedding_service = None
korean_text_processor = None

# AI LLM ì„œë¹„ìŠ¤ URL (ê¸°ì¡´ backend API í™œìš©)
LLM_SERVICE_URL = "http://localhost:8000"

# Pydantic ëª¨ë¸
class GenerateRequest(BaseModel):
    query: str
    context: str = ""
    korean_analysis: Dict[str, Any] = Field(default_factory=dict)
    max_length: int = 500

class GenerateResponse(BaseModel):
    query: str
    response: str
    context_used: str
    korean_features: Dict[str, Any]
    generation_time: float

class SimpleRAGRequest(BaseModel):
    query: str
    user_id: str = "default"
    include_sources: bool = True

class SimpleRAGResponse(BaseModel):
    query: str
    response: str
    sources: List[Dict[str, Any]]
    korean_analysis: Dict[str, Any]

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
async def initialize_services():
    """í•œêµ­ì–´ ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
    global korean_embedding_service, korean_text_processor
    
    try:
        if korean_embedding_service is None:
            korean_embedding_service = KoreanEmbeddingService(embedding_dim=768)
            korean_text_processor = KoreanTextProcessor()
            logger.info("âœ… Korean Embedding Service ì´ˆê¸°í™” ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

@app.on_event("startup")
async def startup_event():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    logger.info("ğŸš€ Korean RAG Service ì‹œì‘ ì¤‘...")
    success = await initialize_services()
    if success:
        logger.info("ğŸ“ Korean RAG Service ì¤€ë¹„ ì™„ë£Œ (Port 8009)")
    else:
        logger.error("âŒ Korean RAG Service ì´ˆê¸°í™” ì‹¤íŒ¨")

@app.get("/")
async def root():
    return {
        "service": "Korean RAG Service",
        "version": "2.0.0-korean-optimized",
        "status": "running",
        "description": "í•œêµ­ì–´ íŠ¹í™” RAG API ì„œë¹„ìŠ¤",
        "features": [
            "TF-IDF ê¸°ë°˜ í•œêµ­ì–´ ì„ë² ë”©",
            "Kiwi í˜•íƒœì†Œ ë¶„ì„",
            "í•œêµ­ì–´ ì»¨í…ìŠ¤íŠ¸ ìƒì„±",
            "LLM í†µí•© ì‘ë‹µ ìƒì„±"
        ]
    }

@app.get("/health")
async def health_check():
    services_ready = korean_embedding_service is not None and korean_text_processor is not None
    
    return {
        "status": "healthy" if services_ready else "initializing",
        "services": {
            "korean_embedding": korean_embedding_service is not None,
            "korean_text_processor": korean_text_processor is not None,
            "llm_backend": True  # ê¸°ì¡´ backend ì„œë¹„ìŠ¤ ì‚¬ìš©
        },
        "timestamp": datetime.now().isoformat()
    }

@app.post("/generate")
async def generate_response(request: GenerateRequest):
    """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ ìµœì í™” ì‘ë‹µ ìƒì„±"""
    try:
        start_time = datetime.now()
        
        if not korean_text_processor:
            await initialize_services()
        
        # 1. ì§ˆì˜ í•œêµ­ì–´ ë¶„ì„
        query_processed = korean_text_processor.preprocess_text(request.query)
        query_tokens = korean_text_processor.tokenize(query_processed)
        query_keywords = korean_text_processor.extract_keywords(query_processed)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
        context_analysis = {}
        if request.context:
            context_processed = korean_text_processor.preprocess_text(request.context)
            context_analysis = {
                "context_length": len(request.context),
                "processed_length": len(context_processed),
                "context_keywords": korean_text_processor.extract_keywords(context_processed)
            }
        
        # 3. í•œêµ­ì–´ íŠ¹ì„± ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        korean_prompt = f"""ë‹¤ìŒì€ í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ì§ˆë¬¸: {request.query}

{f"ì°¸ê³  ë¬¸ì„œ: {request.context}" if request.context else ""}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ {request.max_length}ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        # 4. LLM ì„œë¹„ìŠ¤ í˜¸ì¶œ (ê¸°ì¡´ backend í™œìš©)
        response_text = ""
        try:
            async with httpx.AsyncClient() as client:
                llm_data = {
                    "message": korean_prompt,
                    "user_id": "korean_rag_service"
                }
                response = await client.post(
                    f"{LLM_SERVICE_URL}/api/v1/chat",
                    json=llm_data,
                    timeout=30.0
                )
                if response.status_code == 200:
                    result = response.json()
                    response_text = result.get("response", "")
                else:
                    response_text = "LLM ì„œë¹„ìŠ¤ ì‘ë‹µ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"LLM ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            # í´ë°±: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨ ì‘ë‹µ
            if request.context:
                response_text = f"ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ: {request.context[:300]}..."
            else:
                response_text = "ê´€ë ¨ ë¬¸ì„œê°€ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        generation_time = (datetime.now() - start_time).total_seconds()
        
        return GenerateResponse(
            query=request.query,
            response=response_text,
            context_used=request.context[:200] + "..." if len(request.context) > 200 else request.context,
            korean_features={
                "query_tokens": query_tokens,
                "query_keywords": query_keywords,
                "context_analysis": context_analysis,
                "response_length": len(response_text)
            },
            generation_time=generation_time
        )
        
    except Exception as e:
        logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/simple_rag")
async def simple_rag(request: SimpleRAGRequest):
    """ê°„ë‹¨í•œ RAG ì§ˆì˜ì‘ë‹µ (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì—†ì´ ë…ë¦½ ì‹¤í–‰)"""
    try:
        if not korean_text_processor:
            await initialize_services()
        
        # ì§ˆì˜ ë¶„ì„
        query_processed = korean_text_processor.preprocess_text(request.query)
        korean_analysis = {
            "original_query": request.query,
            "processed_query": query_processed,
            "tokenized": korean_text_processor.tokenize(query_processed),
            "keywords": korean_text_processor.extract_keywords(query_processed)
        }
        
        # ê¸°ë³¸ ì‘ë‹µ (ë²¡í„° ê²€ìƒ‰ ì—†ì´)
        response_text = f"'{request.query}'ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì—°ê²°ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤."
        
        # LLM ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹œë„
        try:
            async with httpx.AsyncClient() as client:
                llm_data = {
                    "message": f"ë‹¤ìŒ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”: {request.query}",
                    "user_id": request.user_id
                }
                response = await client.post(
                    f"{LLM_SERVICE_URL}/api/v1/chat",
                    json=llm_data,
                    timeout=20.0
                )
                if response.status_code == 200:
                    result = response.json()
                    response_text = result.get("response", response_text)
        except Exception as e:
            logger.warning(f"LLM ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨, ê¸°ë³¸ ì‘ë‹µ ì‚¬ìš©: {e}")
        
        return SimpleRAGResponse(
            query=request.query,
            response=response_text,
            sources=[],  # ë²¡í„° ê²€ìƒ‰ ë¯¸êµ¬í˜„ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            korean_analysis=korean_analysis
        )
        
    except Exception as e:
        logger.error(f"Simple RAG ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/status")
async def status():
    """ì„œë¹„ìŠ¤ ìƒíƒœ ìƒì„¸ ì •ë³´"""
    services_ready = korean_embedding_service is not None and korean_text_processor is not None
    
    return {
        "service": "Korean RAG Service",
        "version": "2.0.0-korean-optimized",
        "status": "ready" if services_ready else "initializing",
        "mode": "production",
        "features": {
            "korean_tokenization": True,
            "tfidf_embeddings": True,
            "llm_integration": True,
            "korean_optimization": True
        },
        "dependencies": {
            "pytorch": False,
            "transformers": False,
            "kiwi": True,
            "scikit_learn": True
        }
    }

@app.get("/test")
async def test_korean_processing():
    """í•œêµ­ì–´ ì²˜ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    try:
        if not korean_text_processor:
            await initialize_services()
        
        test_text = "ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. AI ê¸°ìˆ ì„ í™œìš©í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œì…ë‹ˆë‹¤."
        
        processed = korean_text_processor.preprocess_text(test_text)
        tokens = korean_text_processor.tokenize(test_text)
        keywords = korean_text_processor.extract_keywords(test_text)
        
        if korean_embedding_service:
            vector = korean_embedding_service.embed_text(test_text)
            vector_info = {
                "dimension": len(vector),
                "sample": vector[:5].tolist()
            }
        else:
            vector_info = {"error": "embedding service not ready"}
        
        return {
            "test_text": test_text,
            "processed": processed,
            "tokens": tokens[:10],  # ì²˜ìŒ 10ê°œë§Œ
            "keywords": keywords,
            "vector_info": vector_info,
            "status": "success"
        }
        
    except Exception as e:
        return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    print("ğŸš€ Korean RAG Service ì‹œì‘ ì¤‘...")
    print("ğŸ“ í•œêµ­ì–´ íŠ¹í™” TF-IDF + Kiwi ê¸°ë°˜ RAG API")
    print("ğŸ”— Running on http://0.0.0.0:8009")
    print("âœ… PyTorch/Transformers ì˜ì¡´ì„± ì—†ìŒ")
    print("ğŸ¤– ì‹¤ì œ í•œêµ­ì–´ RAG ê¸°ëŠ¥ ì œê³µ (ë”ë¯¸ ëª¨ë“œ ì•„ë‹˜)")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8009,
        log_level="info"
    )