"""
Simplified API with RAG integration for document-based chat
"""
from fastapi import FastAPI, HTTPException, File, Form, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import os
from datetime import datetime, timedelta
import uuid
import google.generativeai as genai
import asyncio
from dotenv import load_dotenv

# RAG service imports
try:
    from app.services.ai.rag_service import RAGService, RAGStrategy
    from app.services.document import DocumentService
    from app.core.database import get_db, create_tables
    from sqlalchemy.ext.asyncio import AsyncSession
    RAG_AVAILABLE = True
    print("ğŸ“š [RAG] RAG services loaded successfully - document-based chat enabled!")
except ImportError as e:
    RAG_AVAILABLE = False
    print(f"âš ï¸ [RAG] RAG services not available: {e} - running in basic mode")

# Docling service import
try:
    from app.services.document.docling_client import DoclingClient
    DOCLING_AVAILABLE = True
    print("ğŸ“„ [DOCLING] Docling client loaded successfully - multi-format document processing enabled!")
except ImportError as e:
    DOCLING_AVAILABLE = False
    print(f"âš ï¸ [DOCLING] Docling client not available: {e} - falling back to basic text processing")

# Alternative document processor import
try:
    from app.services.document.alternative_processor import AlternativeProcessor
    ALT_PROCESSOR_AVAILABLE = True
    print("ğŸ“„ [ALT-PROC] Alternative document processor loaded - local Python library processing enabled!")
except ImportError as e:
    ALT_PROCESSOR_AVAILABLE = False
    print(f"âš ï¸ [ALT-PROC] Alternative processor not available: {e}")

# Korean RAG client import
try:
    import sys
    sys.path.append('/home/ptyoung/work/sdc_i/backend/services')
    from korean_rag_client import get_korean_rag_client
    KOREAN_RAG_AVAILABLE = True
    print("ğŸ‡°ğŸ‡· [KOREAN-RAG] Korean RAG client loaded - Korean document-based RAG enabled!")
except ImportError as e:
    KOREAN_RAG_AVAILABLE = False
    print(f"âš ï¸ [KOREAN-RAG] Korean RAG client not available: {e}")

# Web search service import
try:
    from app.services.web_search import WebSearchService
    WEB_SEARCH_AVAILABLE = True
    print("ğŸŒ [WEB-SEARCH] Web search service loaded - Searxng web search enabled!")
except ImportError as e:
    WEB_SEARCH_AVAILABLE = False
    print(f"âš ï¸ [WEB-SEARCH] Web search service not available: {e}")

# Agentic RAG service import
try:
    from app.services.ai.agentic_rag import AgenticRAGSystem
    AGENTIC_RAG_AVAILABLE = True
    print("ğŸ¤– [AGENTIC-RAG] Agentic RAG system loaded - Advanced AI agent capabilities enabled!")
except ImportError as e:
    AGENTIC_RAG_AVAILABLE = False
    print(f"âš ï¸ [AGENTIC-RAG] Agentic RAG system not available: {e}")

# Guardrails service import
try:
    from app.services.guardrails_client import get_guardrails_client, validate_user_input, validate_ai_output
    GUARDRAILS_AVAILABLE = True
    print("ğŸ›¡ï¸ [GUARDRAILS] Arthur AI Guardrails client loaded - Content safety validation enabled!")
except ImportError as e:
    GUARDRAILS_AVAILABLE = False
    print(f"âš ï¸ [GUARDRAILS] Guardrails client not available: {e} - running without content filtering")

# RAG evaluation service import
try:
    from app.services.rag_evaluation_client import (
        get_rag_evaluation_client, 
        evaluate_rag_session, 
        RAGPerformanceTracker
    )
    RAG_EVALUATION_AVAILABLE = True
    print("ğŸ“Š [RAG-EVAL] RAG Performance Evaluation client loaded - Performance metrics enabled!")
except ImportError as e:
    RAG_EVALUATION_AVAILABLE = False
    print(f"âš ï¸ [RAG-EVAL] RAG evaluation client not available: {e} - running without performance metrics")

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Gemini AI ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    
app = FastAPI(title="SDC Backend - Simple", version="0.1.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001", "http://localhost:3002", "http://localhost:3003"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Simple models
class ChatRequest(BaseModel):
    message: str
    provider: Optional[str] = "gemini"
    use_rag: Optional[bool] = False
    use_web_search: Optional[bool] = False
    web_search_engines: Optional[List[str]] = ["google"]
    search_mode: Optional[str] = "documents"
    use_agentic_rag: Optional[bool] = False
    agentic_complexity_threshold: Optional[int] = 5
    user_id: Optional[str] = "default_user"
    conversation_id: Optional[str] = None
    conversation_history: Optional[List[Dict]] = None
    # Multi-RAG configuration
    enabled_rag_types: Optional[Dict[str, bool]] = {
        "vector": True,
        "graph": True,
        "keyword": True,
        "database": True
    }

class ChatResponse(BaseModel):
    success: bool
    response: str
    provider: Optional[str] = None
    sources: Optional[List[Dict]] = None
    # Multi-RAG results
    rag_results: Optional[List[Dict]] = None
    has_multi_rag: Optional[bool] = False
    conversation_id: Optional[str] = None
    message_id: Optional[str] = None

class ConversationModel(BaseModel):
    id: str
    title: Optional[str] = None
    created_at: str
    updated_at: str
    message_count: int

class MessageModel(BaseModel):
    id: str
    content: str
    role: str
    created_at: str
    conversation_id: str
    metadata: Optional[Dict] = None

class RatingRequest(BaseModel):
    message_id: str
    user_id: str
    rating: int
    feedback: Optional[str] = None

# Mock data storage
conversations_db = {}
messages_db = {}
ratings_db = {}
user_documents = {}  # ì‚¬ìš©ìë³„ ì—…ë¡œë“œëœ ë¬¸ì„œ ì €ì¥ì†Œ

# AI Service Class
class AIService:
    def __init__(self):
        self.gemini_model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-pro")
        self.temperature = float(os.getenv("LLM_TEMPERATURE", "0.7"))
        self.max_tokens = int(os.getenv("LLM_MAX_TOKENS", "4096"))
        self.rag_service = None
        self.document_service = None
        self.web_search_service = None
        self.agentic_rag_system = None
        
        # Initialize RAG services if available
        if RAG_AVAILABLE:
            print("ğŸ”§ [RAG] Initializing RAG services...")
        else:
            print("âš ï¸ [RAG] RAG services not available - using basic AI only")
            
        # Initialize web search service if available
        if WEB_SEARCH_AVAILABLE:
            self.web_search_service = WebSearchService()
            print("ğŸŒ [WEB-SEARCH] Web search service initialized")
            
        # Initialize agentic RAG system if available
        if AGENTIC_RAG_AVAILABLE:
            self.agentic_rag_system = AgenticRAGSystem()
            print("ğŸ¤– [AGENTIC-RAG] Agentic RAG system initialized")
        
    async def generate_gemini_response(self, message: str, provider: str = "gemini", conversation_history: List[Dict] = None) -> str:
        """ğŸ”„ LOCAL LLM MIGRATION POINT 7: ë°±ì—”ë“œ ì‘ë‹µ ìƒì„± ë©”ì†Œë“œ
        í˜„ì¬: Gemini AIë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
        í–¥í›„: ë¡œì»¬ LLMì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±ìœ¼ë¡œ ë³€ê²½
        
        ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš:
        1. ë©”ì†Œë“œëª… ë³€ê²½: generate_gemini_response -> generate_local_llm_response
        2. provider ë§¤ê°œë³€ìˆ˜ í™•ì¥: "gemini" -> "ollama", "vllm", "transformers" ë“±
        3. ëª¨ë¸ ì´ˆê¸°í™” ë°©ì‹ ë³€ê²½:
           - genai.GenerativeModel() -> ollama.Client() ë˜ëŠ” AutoModelForCausalLM.from_pretrained()
        4. API í‚¤ ì²´í¬ -> ë¡œì»¬ ëª¨ë¸ ê°€ìš©ì„± ì²´í¬ë¡œ ë³€ê²½
        5. ì‘ë‹µ ìƒì„± ë°©ì‹ ë³€ê²½:
           - model.generate_content() -> local_llm.generate() ë˜ëŠ” ì§ì ‘ ì¶”ë¡ 
        
        ì˜ˆì‹œ ë³€ê²½ ì‚¬í•­:
        # if provider == "ollama":
        #     client = ollama.Client(host="localhost:11434")
        #     response = client.chat(model="gemma-7b-ko", messages=[...])
        # elif provider == "vllm":
        #     client = OpenAI(base_url="http://localhost:8000/v1")
        #     response = client.chat.completions.create(model="gemma-7b-ko", messages=[...])
        # elif provider == "transformers":
        #     inputs = tokenizer(context_message, return_tensors="pt")
        #     outputs = model.generate(**inputs, max_new_tokens=2048)
        #     response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        """
        print(f"ğŸš€ [GEMINI] Starting AI request for message: {message[:50]}...")
        print(f"ğŸ”‘ [GEMINI] API Key present: {bool(GEMINI_API_KEY)}")
        print(f"âš™ï¸ [GEMINI] Model: {self.gemini_model_name}, Temperature: {self.temperature}")
        
        try:
            if not GEMINI_API_KEY:
                error_msg = "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                print(f"âŒ [GEMINI] {error_msg}")
                return error_msg
            
            # Gemini ëª¨ë¸ ì´ˆê¸°í™”
            print(f"ğŸ“ [GEMINI] Initializing model: {self.gemini_model_name}")
            model = genai.GenerativeModel(self.gemini_model_name)
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
            context_message = message
            if conversation_history and len(conversation_history) > 0:
                print(f"ğŸ“š [GEMINI] Adding conversation history: {len(conversation_history)} messages")
                context = "ì´ì „ ëŒ€í™” ë‚´ìš©:\n"
                for msg in conversation_history[-5:]:  # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨
                    role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ì–´ì‹œìŠ¤í„´íŠ¸"
                    context += f"{role}: {msg['content'][:100]}\n"
                context += f"\ní˜„ì¬ ì§ˆë¬¸: {message}\n\nìœ„ì˜ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."
                context_message = context
                print(f"ğŸ“ [GEMINI] Final context length: {len(context_message)} chars")
            
            # Geminiì—ê²Œ ìš”ì²­
            print(f"ğŸŒ [GEMINI] Sending request to Gemini API...")
            response = await asyncio.to_thread(
                model.generate_content, 
                context_message,
                generation_config=genai.types.GenerationConfig(
                    temperature=self.temperature,
                    max_output_tokens=self.max_tokens,
                )
            )
            
            print(f"ğŸ“¡ [GEMINI] Raw response received: {type(response)}")
            print(f"ğŸ“„ [GEMINI] Response text length: {len(response.text) if response.text else 0}")
            
            if response.text:
                result = response.text.strip()
                print(f"âœ… [GEMINI] Success! Response preview: {result[:100]}...")
                return result
            else:
                error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                print(f"âš ï¸ [GEMINI] Empty response: {error_msg}")
                return error_msg
                
        except Exception as e:
            error_msg = f"AI ì„œë¹„ìŠ¤ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            print(f"âŒ [GEMINI] Exception occurred: {type(e).__name__}: {str(e)}")
            import traceback
            print(f"ğŸ“Š [GEMINI] Full traceback:\n{traceback.format_exc()}")
            return error_msg
    
    async def generate_response(
        self, 
        message: str, 
        provider: str = "gemini", 
        conversation_history: List[Dict] = None,
        use_rag: bool = False,
        use_web_search: bool = False,
        web_search_engines: List[str] = None,
        search_mode: str = "documents",
        use_agentic_rag: bool = False,
        agentic_complexity_threshold: int = 5,
        user_id: str = "default_user",
        rag_tracker = None,
        enabled_rag_types: Dict[str, bool] = None
    ) -> Dict[str, any]:
        """ì„ íƒëœ LLM ì œê³µìì— ë”°ë¼ ì‘ë‹µ ìƒì„± (RAG, ì›¹ ê²€ìƒ‰, ì—ì´ì „í‹± RAG ì§€ì›)"""
        print(f"ğŸ¯ [AI] Generate response - provider: {provider}, use_rag: {use_rag}, use_web_search: {use_web_search}, search_mode: {search_mode}, use_agentic_rag: {use_agentic_rag}")
        
        # Multi-RAG ì‚¬ìš© ì²´í¬ - enabled_rag_typesê°€ ì œê³µëœ ê²½ìš° multi-RAG ì‚¬ìš©
        if enabled_rag_types and any(enabled_rag_types.values()):
            print(f"ğŸ”„ [MULTI-RAG] Multi-RAG enabled with types: {enabled_rag_types}")
            return await self.generate_multi_rag_response(
                message, user_id, provider, conversation_history, enabled_rag_types, rag_tracker
            )
        
        # ì—ì´ì „í‹± RAG ì‚¬ìš© ì²´í¬ (ë³µì¡í•œ ì¿¼ë¦¬ì— ëŒ€í•´ ìë™ í™œì„±í™”)
        if use_agentic_rag or self._should_use_agentic_rag(message, agentic_complexity_threshold):
            return await self._generate_agentic_rag_response(
                message, provider, conversation_history, user_id, 
                use_rag, use_web_search, web_search_engines, search_mode
            )
        
        # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
        context_parts = []
        all_sources = []
        
        # ë¬¸ì„œ ê²€ìƒ‰ (RAG) ì²˜ë¦¬
        if use_rag or search_mode in ['documents', 'combined']:
            print(f"ğŸ“š [RAG] Attempting document-based response")
            
            # RAG ì¶”ì  ì‹œì‘
            if rag_tracker:
                rag_tracker.start_retrieval()
            
            # RAG ì„œë¹„ìŠ¤ê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ìš°ì„  ì‚¬ìš© (Korean RAG ìš°ì„ )
            if KOREAN_RAG_AVAILABLE or RAG_AVAILABLE:
                rag_result = await self.generate_rag_response(message, user_id, provider, conversation_history, rag_tracker)
                if search_mode == 'documents':
                    return rag_result
                # í†µí•© ëª¨ë“œì—ì„œëŠ” ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                context_parts.append(f"ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:\n{rag_result['response']}")
                all_sources.extend(rag_result.get('sources', []))
            else:
                # RAG ì„œë¹„ìŠ¤ ë¶ˆê°€ëŠ¥ ì‹œ ê°„ë‹¨í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‚¬ìš©
                doc_result = await self.generate_simple_document_response(message, user_id, provider, conversation_history, rag_tracker)
                if search_mode == 'documents':
                    return doc_result
                # í†µí•© ëª¨ë“œì—ì„œëŠ” ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                if doc_result['response']:
                    context_parts.append(f"ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:\n{doc_result['response']}")
                    all_sources.extend(doc_result.get('sources', []))
        
        # ì›¹ ê²€ìƒ‰ ì²˜ë¦¬
        if use_web_search or search_mode in ['web', 'combined']:
            print(f"ğŸŒ [WEB-SEARCH] Attempting web search")
            if WEB_SEARCH_AVAILABLE and self.web_search_service:
                try:
                    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
                    search_response = await self.web_search_service.search(
                        query=message,
                        engines=web_search_engines or ['google'],
                        language='ko'
                    )
                    
                    if search_response.results:
                        print(f"ğŸ” [WEB-SEARCH] Found {len(search_response.results)} web results")
                        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
                        web_context = self.web_search_service.format_results_for_context(search_response)
                        context_parts.append(f"ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n{web_context}")
                        
                        # ì›¹ ê²€ìƒ‰ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì§ì ‘ ê²°ê³¼ ë°˜í™˜
                        if search_mode == 'web':
                            enhanced_message = f"ë‹¤ìŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n\n{web_context}\n\nì§ˆë¬¸: {message}"
                            response = await self.generate_gemini_response(enhanced_message, provider, conversation_history)
                            return {
                                "response": response, 
                                "sources": [{"type": "web", "results": search_response.results[:3]}]
                            }
                    else:
                        print(f"âš ï¸ [WEB-SEARCH] No web results found")
                        
                except Exception as e:
                    print(f"âŒ [WEB-SEARCH] Web search error: {e}")
            else:
                print(f"âš ï¸ [WEB-SEARCH] Web search service not available")
        
        # í†µí•© ëª¨ë“œ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° í†µí•© ì‘ë‹µ ìƒì„±
        if context_parts:
            print(f"ğŸ”€ [AI] Generating integrated response with {len(context_parts)} context parts")
            
            # ìƒì„± ë‹¨ê³„ ì¶”ì  ì‹œì‘
            if rag_tracker:
                rag_tracker.start_generation()
            
            combined_context = "\n\n".join(context_parts)
            enhanced_message = f"ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n\n{combined_context}\n\nì§ˆë¬¸: {message}"
            response = await self.generate_gemini_response(enhanced_message, provider, conversation_history)
            
            # ìƒì„± ë‹¨ê³„ ì¶”ì  ì™„ë£Œ
            if rag_tracker:
                rag_tracker.end_generation(response, provider or "gemini")
            
            return {"response": response, "sources": all_sources}
        
        # ê¸°ë³¸ AI ì‘ë‹µ
        print(f"ğŸ¤– [AI] Using basic AI response")
        
        # ìƒì„± ë‹¨ê³„ ì¶”ì  ì‹œì‘ (ê¸°ë³¸ ì‘ë‹µì˜ ê²½ìš°)
        if rag_tracker:
            rag_tracker.start_generation()
        
        if provider == "gemini":
            response = await self.generate_gemini_response(message, provider, conversation_history)
        elif provider == "claude":
            response = "Claude APIëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Geminië¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
        elif provider == "openai":
            response = "OpenAI APIëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Geminië¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
        else:
            response = await self.generate_gemini_response(message, "gemini", conversation_history)
        
        # ìƒì„± ë‹¨ê³„ ì¶”ì  ì™„ë£Œ (ê¸°ë³¸ ì‘ë‹µì˜ ê²½ìš°)
        if rag_tracker:
            rag_tracker.end_generation(response, provider or "gemini")
        
        return {"response": response, "sources": []}
    
    async def generate_rag_response(
        self, 
        message: str, 
        user_id: str,
        provider: str = "gemini",
        conversation_history: List[Dict] = None,
        rag_tracker = None
    ) -> Dict[str, any]:
        """RAGë¥¼ ì‚¬ìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„± - Korean RAG ìš°ì„ """
        print(f"ğŸ“š [RAG] Starting RAG response generation")
        
        # Korean RAGê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©
        if KOREAN_RAG_AVAILABLE:
            print(f"ğŸ‡°ğŸ‡· [RAG] Korean RAG available, using Korean RAG service")
            return await self._generate_korean_rag_response(
                message, provider, conversation_history, user_id, rag_tracker
            )
        
        # Korean RAGê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ RAG ì‹œìŠ¤í…œ ì‚¬ìš©
        try:
            # RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì´ˆê¸°í™”
            if not self.rag_service:
                print(f"ğŸ”§ [RAG] Initializing RAG service...")
                # Mock database session for now - in production use proper DI
                db_session = None  # This would be injected properly
                if db_session:
                    self.rag_service = RAGService(db_session)
                    self.document_service = DocumentService(db_session)
                else:
                    print(f"âŒ [RAG] Database session not available - falling back to simple document search")
                    return await self.generate_simple_document_response(
                        message, user_id, provider, conversation_history, rag_tracker
                    )
            
            # RAG ì¿¼ë¦¬ ìˆ˜í–‰
            print(f"ğŸ” [RAG] Performing RAG query...")
            rag_response = await self.rag_service.query(
                query=message,
                user_id=user_id,
                strategy=RAGStrategy.HYBRID,  # Use hybrid search for best results
            )
            
            print(f"âœ… [RAG] RAG query completed - {len(rag_response.sources)} sources found")
            
            # ì†ŒìŠ¤ ì •ë³´ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            sources = []
            for source in rag_response.sources:
                sources.append({
                    "document_id": source.chunk.document_id,
                    "chunk_id": source.chunk.id,
                    "content_preview": source.chunk.content[:200] + "..." if len(source.chunk.content) > 200 else source.chunk.content,
                    "similarity_score": source.similarity_score,
                    "document_title": source.chunk.document.title if source.chunk.document else "Unknown Document",
                    "metadata": source.metadata
                })
            
            print(f"ğŸ“„ [RAG] Generated response with {len(sources)} sources")
            return {
                "response": rag_response.answer,
                "sources": sources
            }
            
        except Exception as e:
            print(f"âŒ [RAG] Error in RAG response generation: {str(e)}")
            import traceback
            print(f"ğŸ“Š [RAG] Full traceback: {traceback.format_exc()}")
            
            # ì‹¤íŒ¨ì‹œ simple document searchë¡œ fallback
            print(f"ğŸ”„ [RAG] Falling back to simple document search")
            return await self.generate_simple_document_response(
                message, user_id, provider, conversation_history, rag_tracker
            )
    
    async def _generate_korean_rag_response(
        self,
        message: str,
        provider: str,
        conversation_history: List[Dict[str, str]],
        user_id: str,
        rag_tracker=None
    ) -> Dict[str, Any]:
        """Korean RAG ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        print(f"ğŸ‡°ğŸ‡· [KOREAN-RAG] Generating Korean RAG response for user: {user_id}")
        
        try:
            # Korean RAG í´ë¼ì´ì–¸íŠ¸ë¡œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            korean_rag_client = get_korean_rag_client()
            
            # RAG ì¶”ì  - ê²€ìƒ‰ ì‹œì‘
            if rag_tracker:
                rag_tracker.start_retrieval()
            
            search_result = await korean_rag_client.search_context(message)
            
            if search_result.get("status") != "success":
                print(f"ğŸ‡°ğŸ‡· [KOREAN-RAG] Search failed: {search_result.get('message')}, fallback to simple document search")
                return await self.generate_simple_document_response(
                    message, user_id, provider, conversation_history, rag_tracker
                )
            
            has_context = search_result.get("has_context", False)
            context = search_result.get("context", "")
            chunks_count = search_result.get("chunks_count", 0)
            relevant_chunks = search_result.get("relevant_chunks", [])
            
            print(f"ğŸ‡°ğŸ‡· [KOREAN-RAG] Search completed: {chunks_count} chunks found, has_context: {has_context}")
            
            # RAG ì¶”ì  - ê²€ìƒ‰ ë‹¨ê³„ ì™„ë£Œ
            if rag_tracker and relevant_chunks:
                chunks = [
                    {
                        "document_id": chunk.get("document_id", "unknown"),
                        "content": chunk.get("text", ""),
                        "score": chunk.get("similarity_score", 0.0)
                    }
                    for chunk in relevant_chunks
                ]
                rag_tracker.end_retrieval(chunks)
            
            if has_context and context:
                # Korean RAGì—ì„œ ì´ë¯¸ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                rag_prompt = search_result.get("rag_prompt", "")
                
                if rag_prompt:
                    print(f"ğŸ‡°ğŸ‡· [KOREAN-RAG] Using Korean RAG optimized prompt ({len(rag_prompt)} chars)")
                    
                    # RAG ì¶”ì  - ìƒì„± ì‹œì‘
                    if rag_tracker:
                        rag_tracker.start_generation(rag_prompt)
                    
                    # Korean RAG ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ ìƒì„±
                    response = await self.generate_gemini_response(
                        rag_prompt, 
                        provider, 
                        conversation_history=[]  # RAG í”„ë¡¬í”„íŠ¸ëŠ” ì´ë¯¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ë¯€ë¡œ íˆìŠ¤í† ë¦¬ ì œì™¸
                    )
                    
                    # RAG ì¶”ì  - ìƒì„± ì™„ë£Œ
                    if rag_tracker:
                        rag_tracker.end_generation(response)
                    
                    print(f"ğŸ‡°ğŸ‡· [KOREAN-RAG] Korean RAG response generated successfully")
                    
                    # ì†ŒìŠ¤ ì •ë³´ êµ¬ì„±
                    sources = []
                    for chunk in relevant_chunks:
                        sources.append({
                            "document_id": chunk.get("document_id", "unknown"),
                            "chunk_id": chunk.get("chunk_id", 0),
                            "similarity_score": chunk.get("similarity_score", 0.0),
                            "content_preview": chunk.get("text", "")[:200] + "...",
                            "metadata": chunk.get("metadata", {}),
                            "source_type": "korean_rag"
                        })
                    
                    return {
                        "response": response,
                        "sources": sources,
                        "rag_method": "korean_rag",
                        "context_chunks": chunks_count,
                        "similarity_threshold": search_result.get("similarity_threshold", 0.7)
                    }
                else:
                    print(f"ğŸ‡°ğŸ‡· [KOREAN-RAG] No RAG prompt generated, fallback to simple context")
                    # ì»¨í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš° ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    context_prompt = f"ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {message}"
                    
                    # RAG ì¶”ì  - ìƒì„± ì‹œì‘
                    if rag_tracker:
                        rag_tracker.start_generation(context_prompt)
                    
                    response = await self.generate_gemini_response(context_prompt, provider, conversation_history)
                    
                    # RAG ì¶”ì  - ìƒì„± ì™„ë£Œ
                    if rag_tracker:
                        rag_tracker.end_generation(response)
                    
                    return {
                        "response": response,
                        "sources": [{"content_preview": context[:200] + "...", "source_type": "korean_rag"}],
                        "rag_method": "korean_rag_simple"
                    }
            else:
                print(f"ğŸ‡°ğŸ‡· [KOREAN-RAG] No relevant context found, fallback to simple document search")
                return await self.generate_simple_document_response(
                    message, user_id, provider, conversation_history, rag_tracker
                )
                
        except Exception as e:
            print(f"âŒ [KOREAN-RAG] Error in Korean RAG: {str(e)}")
            import traceback
            print(f"ğŸ“Š [KOREAN-RAG] Full traceback: {traceback.format_exc()}")
            
            # ì‹¤íŒ¨ì‹œ simple document searchë¡œ fallback
            print(f"ğŸ”„ [KOREAN-RAG] Falling back to simple document search")
            return await self.generate_simple_document_response(
                message, user_id, provider, conversation_history, rag_tracker
            )
    
    async def generate_multi_rag_response(
        self,
        message: str,
        user_id: str,
        provider: str = "gemini",
        conversation_history: List[Dict] = None,
        enabled_rag_types: Dict[str, bool] = None,
        rag_tracker = None
    ) -> Dict[str, any]:
        """Multiple RAG systemsì„ ì‚¬ìš©í•œ í†µí•© ì‘ë‹µ ìƒì„±"""
        print(f"ğŸ”„ [MULTI-RAG] Starting multi-RAG query for user: {user_id}")
        
        if not enabled_rag_types:
            enabled_rag_types = {"vector": True, "graph": True, "keyword": True, "database": True}
        
        rag_results = []
        successful_responses = []
        
        # Vector RAG (Korean RAG Service)
        if enabled_rag_types.get("vector", False):
            try:
                print(f"ğŸ‡°ğŸ‡· [MULTI-RAG] Attempting Vector RAG...")
                vector_result = await self._generate_korean_rag_response(
                    message, provider, conversation_history, user_id, rag_tracker
                )
                
                if vector_result and vector_result.get("response"):
                    successful_responses.append(vector_result["response"])
                    rag_results.append({
                        "type": "vector",
                        "success": True,
                        "response": vector_result["response"],
                        "metadata": {
                            "sources": len(vector_result.get("sources", [])),
                            "confidence": 0.8,
                            "processingTime": 1.2
                        }
                    })
                    print(f"âœ… [MULTI-RAG] Vector RAG successful")
                else:
                    rag_results.append({
                        "type": "vector",
                        "success": False,
                        "error": "No relevant context found in vector search"
                    })
                    print(f"âŒ [MULTI-RAG] Vector RAG failed - no context")
            except Exception as e:
                rag_results.append({
                    "type": "vector",
                    "success": False,
                    "error": f"Vector RAG error: {str(e)}"
                })
                print(f"âŒ [MULTI-RAG] Vector RAG exception: {e}")
        
        # Graph RAG Service
        if enabled_rag_types.get("graph", False):
            try:
                import httpx
                print(f"ğŸ•¸ï¸ [MULTI-RAG] Attempting Graph RAG...")
                async with httpx.AsyncClient(timeout=10.0) as client:
                    graph_response = await client.post(
                        "http://localhost:8008/query",
                        json={"query": message, "user_id": user_id}
                    )
                    if graph_response.status_code == 200:
                        graph_data = graph_response.json()
                        if graph_data.get("success") and graph_data.get("response"):
                            successful_responses.append(graph_data["response"])
                            rag_results.append({
                                "type": "graph",
                                "success": True,
                                "response": graph_data["response"],
                                "metadata": {
                                    "resultCount": graph_data.get("result_count", 0),
                                    "confidence": 0.75,
                                    "processingTime": graph_data.get("processing_time", 0)
                                }
                            })
                            print(f"âœ… [MULTI-RAG] Graph RAG successful")
                        else:
                            rag_results.append({
                                "type": "graph",
                                "success": False,
                                "error": "No relevant graph relationships found"
                            })
                    else:
                        rag_results.append({
                            "type": "graph",
                            "success": False,
                            "error": f"Graph RAG service error: HTTP {graph_response.status_code}"
                        })
            except Exception as e:
                rag_results.append({
                    "type": "graph",
                    "success": False,
                    "error": f"Graph RAG connection error: {str(e)}"
                })
                print(f"âŒ [MULTI-RAG] Graph RAG exception: {e}")
        
        # Keyword RAG Service
        if enabled_rag_types.get("keyword", False):
            try:
                import httpx
                print(f"ğŸ” [MULTI-RAG] Attempting Keyword RAG...")
                async with httpx.AsyncClient(timeout=10.0) as client:
                    keyword_response = await client.post(
                        "http://localhost:8011/search",
                        json={"query": message, "user_id": user_id}
                    )
                    if keyword_response.status_code == 200:
                        keyword_data = keyword_response.json()
                        if keyword_data.get("success") and keyword_data.get("response"):
                            successful_responses.append(keyword_data["response"])
                            rag_results.append({
                                "type": "keyword",
                                "success": True,
                                "response": keyword_data["response"],
                                "metadata": {
                                    "resultCount": keyword_data.get("result_count", 0),
                                    "confidence": 0.7,
                                    "processingTime": keyword_data.get("processing_time", 0)
                                }
                            })
                            print(f"âœ… [MULTI-RAG] Keyword RAG successful")
                        else:
                            rag_results.append({
                                "type": "keyword",
                                "success": False,
                                "error": "No relevant keywords found"
                            })
                    else:
                        rag_results.append({
                            "type": "keyword",
                            "success": False,
                            "error": f"Keyword RAG service error: HTTP {keyword_response.status_code}"
                        })
            except Exception as e:
                rag_results.append({
                    "type": "keyword",
                    "success": False,
                    "error": f"Keyword RAG connection error: {str(e)}"
                })
                print(f"âŒ [MULTI-RAG] Keyword RAG exception: {e}")
        
        # Database RAG (Text-to-SQL)
        if enabled_rag_types.get("database", False):
            try:
                import httpx
                print(f"ğŸ—„ï¸ [MULTI-RAG] Attempting Database RAG...")
                async with httpx.AsyncClient(timeout=15.0) as client:
                    db_response = await client.post(
                        "http://localhost:8012/ask",
                        json={"question": message, "user_id": user_id}
                    )
                    if db_response.status_code == 200:
                        db_data = db_response.json()
                        if db_data.get("success") and db_data.get("data"):
                            # Extract meaningful response from database RAG
                            final_answer = db_data["data"].get("final_answer", {})
                            suggested_prompt = final_answer.get("suggested_prompt", "")
                            if suggested_prompt:
                                successful_responses.append(suggested_prompt)
                                rag_results.append({
                                    "type": "database",
                                    "success": True,
                                    "response": suggested_prompt,
                                    "metadata": {
                                        "resultCount": db_data["data"].get("query_execution", {}).get("row_count", 0),
                                        "confidence": db_data["data"].get("rag_processing", {}).get("quality_score", 0.0),
                                        "processingTime": db_data.get("processing_time", 0)
                                    }
                                })
                                print(f"âœ… [MULTI-RAG] Database RAG successful")
                            else:
                                rag_results.append({
                                    "type": "database",
                                    "success": False,
                                    "error": "No database results found"
                                })
                        else:
                            rag_results.append({
                                "type": "database",
                                "success": False,
                                "error": db_data.get("error", "Database RAG query failed")
                            })
                    else:
                        rag_results.append({
                            "type": "database",
                            "success": False,
                            "error": f"Database RAG service error: HTTP {db_response.status_code}"
                        })
            except Exception as e:
                rag_results.append({
                    "type": "database",
                    "success": False,
                    "error": f"Database RAG connection error: {str(e)}"
                })
                print(f"âŒ [MULTI-RAG] Database RAG exception: {e}")
        
        # Generate combined response using successful results
        if successful_responses:
            # Create a comprehensive prompt combining all RAG results
            combined_context = "\n\n".join([
                f"RAG ê²€ìƒ‰ ê²°ê³¼ {i+1}:\n{response}" 
                for i, response in enumerate(successful_responses)
            ])
            
            enhanced_prompt = f"""ë‹¤ìŒì€ ì—¬ëŸ¬ RAG ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ëœ ì •ë³´ì…ë‹ˆë‹¤:

{combined_context}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ '{message}'ì— ëŒ€í•´ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
ì¤‘ë³µë˜ëŠ” ì •ë³´ëŠ” í†µí•©í•˜ê³ , ìƒì¶©ë˜ëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´ ì‹ ë¢°ë„ê°€ ë†’ì€ ì •ë³´ë¥¼ ìš°ì„ ì‹œí•˜ì„¸ìš”."""
            
            try:
                combined_response = await self.generate_gemini_response(
                    enhanced_prompt, provider, conversation_history
                )
                
                print(f"âœ… [MULTI-RAG] Successfully generated combined response using {len(successful_responses)} RAG sources")
                return {
                    "response": combined_response,
                    "rag_results": rag_results,
                    "has_multi_rag": True,
                    "successful_rag_count": len(successful_responses)
                }
                
            except Exception as e:
                print(f"âŒ [MULTI-RAG] Failed to generate combined response: {e}")
                # Fallback to first successful response
                return {
                    "response": successful_responses[0],
                    "rag_results": rag_results,
                    "has_multi_rag": True,
                    "successful_rag_count": len(successful_responses)
                }
        else:
            # No successful RAG responses, fallback to basic AI
            print(f"âŒ [MULTI-RAG] No successful RAG responses, falling back to basic AI")
            basic_response = await self.generate_gemini_response(message, provider, conversation_history)
            return {
                "response": basic_response,
                "rag_results": rag_results,
                "has_multi_rag": True,
                "successful_rag_count": 0
            }
    
    async def generate_simple_document_response(
        self, 
        message: str, 
        user_id: str,
        provider: str = "gemini",
        conversation_history: List[Dict] = None,
        rag_tracker = None
    ) -> Dict[str, any]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ ì‘ë‹µ ìƒì„±"""
        print(f"ğŸ“š [SIMPLE-RAG] Starting simple document search for user: {user_id}")
        
        try:
            # ì‚¬ìš©ìì˜ ì—…ë¡œë“œëœ ë¬¸ì„œ ê²€ìƒ‰
            user_docs = user_documents.get(user_id, [])
            if not user_docs:
                print(f"ğŸ“š [SIMPLE-RAG] No documents found for user {user_id}")
                response = await self.generate_gemini_response(message, provider, conversation_history)
                return {"response": response, "sources": []}
            
            print(f"ğŸ“š [SIMPLE-RAG] Found {len(user_docs)} documents for user {user_id}")
            
            # ìŠ¤ë§ˆíŠ¸ ë¬¸ì„œ ì²˜ë¦¬: ì „ì²´ ë‚´ìš© í™œìš© + í‚¤ì›Œë“œ ê°•ì¡°
            relevant_content = []
            search_terms = message.lower().split()
            
            for doc in user_docs:
                doc_content = ""
                if isinstance(doc["content"], bytes):
                    try:
                        doc_content = doc["content"].decode('utf-8')
                    except:
                        doc_content = str(doc["content"])
                else:
                    doc_content = str(doc["content"])
                
                doc_content_lower = doc_content.lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                keyword_matches = 0
                matched_terms = []
                for term in search_terms:
                    if len(term) > 2 and term in doc_content_lower:
                        keyword_matches += 1
                        matched_terms.append(term)
                
                # ë¬¸ì„œ ê¸¸ì´ì— ë”°ë¥¸ ì²˜ë¦¬
                max_content_length = 2000  # AIê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´
                
                if len(doc_content) <= max_content_length:
                    # ì§§ì€ ë¬¸ì„œ: ì „ì²´ ë‚´ìš© ì œê³µ
                    content_to_use = doc_content
                    processing_note = "ì „ì²´ ë¬¸ì„œ ë‚´ìš©"
                else:
                    # ê¸´ ë¬¸ì„œ: í‚¤ì›Œë“œ ì£¼ë³€ í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ + ë¬¸ì„œ ì‹œì‘ ë¶€ë¶„
                    if keyword_matches > 0:
                        # í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°: í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                        best_match_pos = doc_content_lower.find(matched_terms[0])
                        start = max(0, best_match_pos - 500)  # ì• 500ì
                        end = min(len(doc_content), best_match_pos + 1500)  # ë’¤ 1500ì
                        
                        # ë¬¸ì„œ ì‹œì‘ ë¶€ë¶„ë„ í¬í•¨ (ì œëª©, ê°œìš” ë“±)
                        beginning = doc_content[:300] if start > 300 else ""
                        middle_content = doc_content[start:end]
                        
                        content_to_use = f"{beginning}\n\n...[ê´€ë ¨ ë¶€ë¶„]...\n\n{middle_content}"
                        processing_note = f"í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ (í‚¤ì›Œë“œ: {', '.join(matched_terms)})"
                    else:
                        # í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°: ë¬¸ì„œ ì•ë¶€ë¶„ ì œê³µ
                        content_to_use = doc_content[:max_content_length]
                        processing_note = "ë¬¸ì„œ ì‹œì‘ ë¶€ë¶„"
                
                relevant_content.append({
                    "document_id": doc["id"],
                    "filename": doc["filename"],
                    "content_preview": content_to_use,
                    "keyword_matches": keyword_matches,
                    "matched_terms": matched_terms,
                    "processing_note": processing_note,
                    "full_length": len(doc_content)
                })
                
                print(f"ğŸ“š [SIMPLE-RAG] Processed {doc['filename']}: {len(content_to_use)} chars, {keyword_matches} keyword matches")
            
            print(f"ğŸ“š [SIMPLE-RAG] Found {len(relevant_content)} relevant content snippets")
            
            # RAG ì¶”ì  - ê²€ìƒ‰ ë‹¨ê³„ ì™„ë£Œ
            if rag_tracker and relevant_content:
                # Convert relevant content to chunks format for tracking
                chunks = [
                    {
                        "document_id": content["document_id"],
                        "content": content["content_preview"],
                        "score": content["keyword_matches"] / max(len(content["matched_terms"]), 1)
                    }
                    for content in relevant_content
                ]
                rag_tracker.end_retrieval(chunks)
            
            # ëª¨ë“  ë¬¸ì„œ ë‚´ìš©ì„ AIì— ì œê³µ (í‚¤ì›Œë“œ ë§¤ì¹­ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
            if relevant_content:
                # ë¬¸ì„œë³„ ìƒì„¸ ì •ë³´ í¬í•¨í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context_parts = []
                total_chars = 0
                
                for content in relevant_content:
                    doc_info = f"=== íŒŒì¼: {content['filename']} ===\n"
                    doc_info += f"ì²˜ë¦¬ ë°©ì‹: {content['processing_note']}\n"
                    if content['keyword_matches'] > 0:
                        doc_info += f"ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(content['matched_terms'])}\n"
                    doc_info += f"ì›ë³¸ í¬ê¸°: {content['full_length']} ë¬¸ì\n\n"
                    doc_info += f"ë‚´ìš©:\n{content['content_preview']}\n"
                    
                    context_parts.append(doc_info)
                    total_chars += len(doc_info)
                
                context = "\n\n".join(context_parts)
                
                # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                enhanced_message = f"""ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì¤‘ìš”í•œ ì§€ì¹¨:
1. ìœ„ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
3. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
4. ë¬¸ì„œì˜ êµ¬ì¡°ì™€ ì„¹ì…˜ì„ íŒŒì•…í•˜ì—¬ ì „ì²´ì ì¸ ë§¥ë½ì„ ì´í•´í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {message}"""
                
                print(f"ğŸ“š [SIMPLE-RAG] Generated enhanced prompt: {len(context)} chars context, {len(relevant_content)} documents")
                
                # ìƒì„± ë‹¨ê³„ ì¶”ì  ì‹œì‘
                if rag_tracker:
                    rag_tracker.start_generation()
                
                response = await self.generate_gemini_response(enhanced_message, provider, conversation_history)
                
                # ìƒì„± ë‹¨ê³„ ì¶”ì  ì™„ë£Œ
                if rag_tracker:
                    rag_tracker.end_generation(response, provider or "gemini")
                
                # ì†ŒìŠ¤ ì •ë³´ ì •ë¦¬
                sources = []
                for content in relevant_content:
                    sources.append({
                        "document_id": content["document_id"],
                        "filename": content["filename"],
                        "content_preview": content["content_preview"][:200] + "...",
                        "keyword_matches": content["keyword_matches"],
                        "processing_note": content["processing_note"]
                    })
                
                return {
                    "response": response,
                    "sources": sources
                }
            else:
                print(f"ğŸ“š [SIMPLE-RAG] No documents available, using basic response")
                response = await self.generate_gemini_response(message, provider, conversation_history)
                return {"response": response, "sources": []}
                
        except Exception as e:
            print(f"âŒ [SIMPLE-RAG] Error in simple document search: {str(e)}")
            import traceback
            print(f"ğŸ“Š [SIMPLE-RAG] Full traceback: {traceback.format_exc()}")
            
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ AIë¡œ fallback
            print(f"ğŸ”„ [SIMPLE-RAG] Falling back to basic AI response")
            response = await self.generate_gemini_response(message, provider, conversation_history)
            return {"response": response, "sources": []}
    
    def _should_use_agentic_rag(self, message: str, complexity_threshold: int) -> bool:
        """ì¿¼ë¦¬ê°€ ì—ì´ì „í‹± RAGë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨"""
        if not AGENTIC_RAG_AVAILABLE or not self.agentic_rag_system:
            return False
        
        # ë³µì¡ë„ í‚¤ì›Œë“œë“¤
        complexity_indicators = [
            'ë¹„êµ', 'ì°¨ì´', 'ì¥ë‹¨ì ', 'ê´€ê³„', 'ì˜í–¥', 'ì›ì¸', 'ê²°ê³¼', 'ë¶„ì„',
            'ì¢…í•©', 'ìš”ì•½', 'ì •ë¦¬', 'ì„¤ëª…', 'ì–´ë–»ê²Œ', 'ì™œ', 'ë¬´ì—‡', 'ì–´ë–¤',
            'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë‚˜', 'ë°˜ë©´ì—'
        ]
        
        # ë³µì¡í•œ ì§ˆë¬¸ íŒ¨í„´
        complex_patterns = [
            message.count('?') > 1,  # ì—¬ëŸ¬ ì§ˆë¬¸
            len(message.split()) > 15,  # ê¸´ ì¿¼ë¦¬
            any(word in message for word in complexity_indicators),
            message.count(',') > 2,  # ì—¬ëŸ¬ ìš”ì†Œ
        ]
        
        complexity_score = sum(complex_patterns) + len(message) / 50
        
        print(f"ğŸ¤– [AGENTIC-RAG] ë³µì¡ë„ ì ìˆ˜: {complexity_score:.1f}, ì„ê³„ê°’: {complexity_threshold}")
        
        return complexity_score >= complexity_threshold
    
    async def _generate_agentic_rag_response(
        self,
        message: str,
        provider: str,
        conversation_history: List[Dict],
        user_id: str,
        use_rag: bool,
        use_web_search: bool,
        web_search_engines: List[str],
        search_mode: str
    ) -> Dict[str, any]:
        """ì—ì´ì „í‹± RAGë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ì‘ë‹µ ìƒì„±"""
        
        if not AGENTIC_RAG_AVAILABLE or not self.agentic_rag_system:
            print(f"âš ï¸ [AGENTIC-RAG] ì—ì´ì „í‹± RAG ì‹œìŠ¤í…œì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥, ê¸°ë³¸ RAGë¡œ ëŒ€ì²´")
            return await self._fallback_to_basic_rag(message, provider, conversation_history, user_id, use_rag, use_web_search, web_search_engines, search_mode)
        
        try:
            print(f"ğŸ¤– [AGENTIC-RAG] ì—ì´ì „í‹± RAG ì‘ë‹µ ìƒì„± ì‹œì‘")
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¤€ë¹„
            context = {
                "use_rag": use_rag,
                "use_web_search": use_web_search,
                "web_search_engines": web_search_engines,
                "search_mode": search_mode,
                "conversation_history": conversation_history,
                "provider": provider
            }
            
            # ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
            plan = await self.agentic_rag_system.plan_execution(message, context)
            print(f"ğŸ¯ [AGENTIC-RAG] ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {len(plan.execution_strategy)}ê°œ ì•¡ì…˜")
            print(f"ğŸ“‹ [AGENTIC-RAG] ì•¡ì…˜ ëª©ë¡: {[a.value for a in plan.execution_strategy]}")
            
            # ê³„íš ì‹¤í–‰
            result = await self.agentic_rag_system.execute_plan(plan, user_id)
            
            print(f"âœ… [AGENTIC-RAG] ì—ì´ì „í‹± RAG ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            print(f"ğŸ“Š [AGENTIC-RAG] ì‹ ë¢°ë„: {result['confidence']:.2f}")
            print(f"ğŸ“š [AGENTIC-RAG] ì†ŒìŠ¤ ìˆ˜: {len(result['sources'])}")
            
            return {
                "response": result["answer"],
                "sources": result["sources"],
                "metadata": {
                    "agentic_rag_used": True,
                    "execution_plan": plan.dict(),
                    "confidence": result["confidence"],
                    "execution_results": len(result["execution_results"])
                }
            }
            
        except Exception as e:
            print(f"âŒ [AGENTIC-RAG] ì—ì´ì „í‹± RAG ì˜¤ë¥˜: {str(e)}")
            print(f"ğŸ”„ [AGENTIC-RAG] ê¸°ë³¸ RAGë¡œ ëŒ€ì²´")
            
            return await self._fallback_to_basic_rag(message, provider, conversation_history, user_id, use_rag, use_web_search, web_search_engines, search_mode)
    
    async def _fallback_to_basic_rag(
        self,
        message: str,
        provider: str,
        conversation_history: List[Dict],
        user_id: str,
        use_rag: bool,
        use_web_search: bool,
        web_search_engines: List[str],
        search_mode: str
    ) -> Dict[str, any]:
        """ì—ì´ì „í‹± RAG ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ RAGë¡œ ëŒ€ì²´"""
        
        # ê¸°ì¡´ generate_response ë¡œì§ì„ ì¬ê·€ í˜¸ì¶œí•˜ì§€ ì•Šê³  ì§ì ‘ êµ¬í˜„
        context_parts = []
        all_sources = []
        
        # ë¬¸ì„œ ê²€ìƒ‰ ì²˜ë¦¬
        if use_rag or search_mode in ['documents', 'combined']:
            if RAG_AVAILABLE:
                rag_result = await self.generate_rag_response(message, user_id, provider, conversation_history)
                context_parts.append(f"ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:\n{rag_result['response']}")
                all_sources.extend(rag_result.get('sources', []))
            else:
                doc_result = await self.generate_simple_document_response(message, user_id, provider, conversation_history)
                if doc_result['response']:
                    context_parts.append(f"ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:\n{doc_result['response']}")
                    all_sources.extend(doc_result.get('sources', []))
        
        # ì›¹ ê²€ìƒ‰ ì²˜ë¦¬
        if use_web_search or search_mode in ['web', 'combined']:
            if WEB_SEARCH_AVAILABLE and self.web_search_service:
                try:
                    search_response = await self.web_search_service.search(
                        query=message,
                        engines=web_search_engines or ['google'],
                        language='ko'
                    )
                    
                    if search_response.results:
                        web_context = self.web_search_service.format_results_for_context(search_response)
                        context_parts.append(f"ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n{web_context}")
                        all_sources.append({"type": "web", "results": search_response.results[:3]})
                except Exception as e:
                    print(f"âŒ [WEB-SEARCH] ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # í†µí•© ì‘ë‹µ ìƒì„±
        if context_parts:
            combined_context = "\n\n".join(context_parts)
            enhanced_message = f"ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n\n{combined_context}\n\nì§ˆë¬¸: {message}"
            response = await self.generate_gemini_response(enhanced_message, provider, conversation_history)
            return {"response": response, "sources": all_sources}
        
        # ê¸°ë³¸ AI ì‘ë‹µ
        response = await self.generate_gemini_response(message, provider, conversation_history)
        return {"response": response, "sources": []}

# AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
ai_service = AIService()

@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "message": "SDC Backend API is running",
        "status": "healthy",
        "version": "0.1.0"
    }

@app.get("/api/v1/providers")
async def get_providers():
    """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì ëª©ë¡ ë°˜í™˜"""
    return {
        "providers": [
            {
                "name": "gemini",
                "display_name": "Google Gemini",
                "available": bool(os.getenv("GEMINI_API_KEY")),
                "models": ["gemini-pro"]
            },
            {
                "name": "claude",
                "display_name": "Anthropic Claude",
                "available": bool(os.getenv("ANTHROPIC_API_KEY")),
                "models": ["claude-3-sonnet"]
            }
        ]
    }


@app.get("/api/v1/conversations/{user_id}")
async def get_user_conversations(user_id: str, limit: int = 10, offset: int = 0):
    """ì‚¬ìš©ìì˜ ëŒ€í™” ëª©ë¡ ë°˜í™˜"""
    
    # ëŒ€í™”ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    if not conversations_db:
        for i in range(1, 8):
            conv_id = f"conv-{i}"
            created_time = datetime.now() - timedelta(days=i-1)
            conversations_db[conv_id] = {
                "id": conv_id,
                "title": f"SDC AIì™€ì˜ ëŒ€í™” {i}" if i > 1 else "ì•ˆë…•í•˜ì„¸ìš”! ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?",
                "created_at": created_time.isoformat(),
                "updated_at": created_time.isoformat(),
                "message_count": 2 + (i % 3)
            }
            
            # ê° ëŒ€í™”ì— ë©”ì‹œì§€ë„ ìƒì„±
            messages_db[conv_id] = [
                {
                    "id": f"msg-{conv_id}-1",
                    "content": f"ì•ˆë…•í•˜ì„¸ìš”! SDC AIì—ê²Œ {i}ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤." if i > 1 else "ì•ˆë…•í•˜ì„¸ìš”! SDCì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤. ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!",
                    "role": "user",
                    "created_at": created_time.isoformat(),
                    "conversation_id": conv_id
                },
                {
                    "id": f"msg-{conv_id}-2",
                    "content": f"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” SDC AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. {i}ë²ˆì§¸ ëŒ€í™”ì—ì„œ ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                    "role": "assistant",
                    "created_at": (created_time + timedelta(seconds=30)).isoformat(),
                    "conversation_id": conv_id,
                    "metadata": {
                        "model": "gemini-pro",
                        "processing_time": 1.2,
                        "confidence": 0.95
                    }
                }
            ]
    
    # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
    conv_list = list(conversations_db.values())
    conv_list.sort(key=lambda x: x["updated_at"], reverse=True)
    
    paginated = conv_list[offset:offset + limit]
    return paginated

@app.get("/api/v1/conversations/{conversation_id}/messages")
async def get_conversation_messages(conversation_id: str):
    """íŠ¹ì • ëŒ€í™”ì˜ ë©”ì‹œì§€ë“¤ ë°˜í™˜"""
    if conversation_id not in messages_db:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    return messages_db[conversation_id]

@app.post("/api/v1/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ - ì‹¤ì œ AIë¥¼ ì‚¬ìš©í•œ ëŒ€í™” ìƒì„±"""
    print(f"\nğŸ¯ [CHAT] New chat request received")
    print(f"ğŸ“ [CHAT] Message: {request.message[:50]}...")
    print(f"ğŸ¤– [CHAT] Provider: {request.provider}")
    print(f"ğŸ‘¤ [CHAT] User: {request.user_id}")
    print(f"ğŸ’¬ [CHAT] Conversation ID: {request.conversation_id}")
    print(f"ğŸ“š [CHAT] History length: {len(request.conversation_history) if request.conversation_history else 0}")
    
    try:
        # ëŒ€í™” ID ìƒì„± ë˜ëŠ” ê¸°ì¡´ ëŒ€í™” ì‚¬ìš©
        conv_id = request.conversation_id or f"conv-{str(uuid.uuid4())[:8]}"
        msg_id = f"msg-{str(uuid.uuid4())[:8]}"
        
        print(f"ğŸ†” [CHAT] Final conversation ID: {conv_id}")
        print(f"ğŸ†” [CHAT] Message ID: {msg_id}")
        
        # ì‚¬ìš©ì ì…ë ¥ ì•ˆì „ì„± ê²€ì¦ (Guardrails)
        if GUARDRAILS_AVAILABLE:
            print(f"ğŸ›¡ï¸ [GUARDRAILS] Validating user input...")
            is_safe, filtered_or_reason = await validate_user_input(
                request.message, 
                request.user_id or "default_user"
            )
            
            if not is_safe:
                print(f"ğŸš« [GUARDRAILS] User input blocked: {filtered_or_reason}")
                raise HTTPException(
                    status_code=400, 
                    detail=f"Content violates safety guidelines: {filtered_or_reason}"
                )
            
            # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ ì‚¬ìš©
            validated_message = filtered_or_reason if filtered_or_reason != request.message else request.message
            print(f"âœ… [GUARDRAILS] User input validated")
        else:
            validated_message = request.message
            print(f"âš ï¸ [GUARDRAILS] Validation skipped - service unavailable")
        
        # RAG ì„±ëŠ¥ ì¶”ì  ì´ˆê¸°í™”
        rag_tracker = None
        if RAG_EVALUATION_AVAILABLE and (request.use_rag or request.search_mode in ['documents', 'combined']):
            session_id = f"session-{conv_id}-{msg_id}"
            rag_tracker = RAGPerformanceTracker(
                session_id=session_id,
                query=validated_message,
                user_id=request.user_id or "default_user"
            )
            print(f"ğŸ“Š [RAG-EVAL] RAG performance tracker initialized for session: {session_id}")

        # ì‹¤ì œ AI ì‘ë‹µ ìƒì„± (RAG ë° ì›¹ ê²€ìƒ‰ ì§€ì›)
        print(f"ğŸš€ [CHAT] Calling AI service with RAG and web search support...")
        ai_result = await ai_service.generate_response(
            message=validated_message,
            provider=request.provider or "gemini",
            conversation_history=request.conversation_history,
            use_rag=request.use_rag or KOREAN_RAG_AVAILABLE,
            use_web_search=request.use_web_search or False,
            web_search_engines=request.web_search_engines or ['google'],
            search_mode=request.search_mode or 'documents',
            use_agentic_rag=request.use_agentic_rag or False,
            agentic_complexity_threshold=request.agentic_complexity_threshold or 5,
            user_id=request.user_id or "default_user",
            rag_tracker=rag_tracker,  # Pass tracker to AI service
            enabled_rag_types=request.enabled_rag_types  # Pass multi-RAG selection
        )
        
        # ê²°ê³¼ì—ì„œ ì‘ë‹µê³¼ ì†ŒìŠ¤ ë¶„ë¦¬
        ai_response = ai_result["response"]
        ai_sources = ai_result.get("sources", [])
        
        print(f"âœ… [CHAT] AI response received!")
        print(f"ğŸ“„ [CHAT] Response length: {len(ai_response)} chars")
        print(f"ğŸ” [CHAT] Response preview: {ai_response[:100]}...")
        
        # AI ì¶œë ¥ ì•ˆì „ì„± ê²€ì¦ (Guardrails)
        if GUARDRAILS_AVAILABLE:
            print(f"ğŸ›¡ï¸ [GUARDRAILS] Validating AI output...")
            is_safe, filtered_or_reason = await validate_ai_output(
                ai_response, 
                request.user_id or "default_user"
            )
            
            if not is_safe:
                print(f"ğŸš« [GUARDRAILS] AI output blocked: {filtered_or_reason}")
                # AI ì¶œë ¥ì´ ì°¨ë‹¨ëœ ê²½ìš° ì•ˆì „í•œ ë©”ì‹œì§€ë¡œ ëŒ€ì²´
                ai_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì•ˆì „ ì •ì±…ì— ë”°ë¼ ì´ ì‘ë‹µì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
            else:
                # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ ì‚¬ìš©
                ai_response = filtered_or_reason if filtered_or_reason != ai_response else ai_response
                print(f"âœ… [GUARDRAILS] AI output validated")
        else:
            print(f"âš ï¸ [GUARDRAILS] AI output validation skipped - service unavailable")
        
        # RAG ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰ (ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ì†ë„ì— ì˜í–¥ ì—†ìŒ)
        if RAG_EVALUATION_AVAILABLE and rag_tracker and ai_sources:
            try:
                print(f"ğŸ“Š [RAG-EVAL] Performing RAG evaluation...")
                # Evaluate without blocking the response
                asyncio.create_task(rag_tracker.evaluate())
                print(f"ğŸ“Š [RAG-EVAL] RAG evaluation task created and running in background")
            except Exception as e:
                print(f"âš ï¸ [RAG-EVAL] RAG evaluation failed: {e}")
        elif RAG_EVALUATION_AVAILABLE and rag_tracker:
            print(f"ğŸ“Š [RAG-EVAL] Skipping evaluation - no sources found")
        else:
            print(f"âš ï¸ [RAG-EVAL] RAG evaluation skipped - service unavailable or no tracker")
        
        # ëŒ€í™” ë° ë©”ì‹œì§€ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë©”ëª¨ë¦¬ ì €ì¥)
        if conv_id not in conversations_db:
            conversations_db[conv_id] = {
                "id": conv_id,
                "title": request.message[:50] + "..." if len(request.message) > 50 else request.message,
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "message_count": 2
            }
            messages_db[conv_id] = []
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        user_msg = {
            "id": f"msg-user-{str(uuid.uuid4())[:8]}",
            "content": request.message,
            "role": "user",
            "created_at": datetime.now().isoformat(),
            "conversation_id": conv_id
        }
        
        # AI ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
        ai_msg = {
            "id": msg_id,
            "content": ai_response,
            "role": "assistant",
            "created_at": datetime.now().isoformat(),
            "conversation_id": conv_id,
            "metadata": {
                "model": request.provider or "gemini",
                "processing_time": 1.5,
                "confidence": 0.95
            }
        }
        
        # ë©”ì‹œì§€ë“¤ì„ ëŒ€í™”ì— ì¶”ê°€
        if conv_id in messages_db:
            messages_db[conv_id].extend([user_msg, ai_msg])
            conversations_db[conv_id]["message_count"] = len(messages_db[conv_id])
            conversations_db[conv_id]["updated_at"] = datetime.now().isoformat()
        
        # ìµœì¢… ì‘ë‹µ ì¤€ë¹„ (ì†ŒìŠ¤ ì •ë³´ ë° Multi-RAG ê²°ê³¼ í¬í•¨)
        final_response = ChatResponse(
            success=True,
            response=ai_response,
            provider=request.provider,
            sources=ai_sources,  # RAG ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
            conversation_id=conv_id,
            message_id=msg_id,
            # Multi-RAG ê²°ê³¼ í¬í•¨
            rag_results=ai_result.get("rag_results"),
            has_multi_rag=ai_result.get("has_multi_rag", False)
        )
        
        # ì†ŒìŠ¤ ì •ë³´ ë¡œê¹…
        if ai_sources and len(ai_sources) > 0:
            print(f"ğŸ“š [CHAT] Response includes {len(ai_sources)} source documents")
            for i, source in enumerate(ai_sources[:3]):  # ì²˜ìŒ 3ê°œë§Œ ë¡œê¹…
                print(f"  ğŸ“„ [CHAT] Source {i+1}: {source.get('document_title', 'Unknown')} (score: {source.get('similarity_score', 0):.3f})")
        else:
            print(f"ğŸ“ [CHAT] Response generated without document sources")
        
        print(f"ğŸ‰ [CHAT] Success! Returning response to frontend")
        print(f"ğŸ“Š [CHAT] Final response: success={final_response.success}, provider={final_response.provider}")
        print(f"ğŸ“ [CHAT] Response content length: {len(final_response.response)}")
        print(f"ğŸ¯ [CHAT] === CHAT REQUEST COMPLETED ===\n")
        
        return final_response
        
    except Exception as e:
        error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(f"âŒ [CHAT] Endpoint error: {type(e).__name__}: {str(e)}")
        import traceback
        print(f"ğŸ“Š [CHAT] Full traceback:\n{traceback.format_exc()}")
        
        error_response = ChatResponse(
            success=False,
            response=error_msg,
            provider=request.provider,
            conversation_id=conv_id if 'conv_id' in locals() else None,
            message_id=msg_id if 'msg_id' in locals() else None
        )
        
        print(f"ğŸ’¥ [CHAT] Returning error response to frontend")
        print(f"ğŸ¯ [CHAT] === CHAT REQUEST FAILED ===\n")
        
        return error_response

@app.post("/api/v1/messages/rate")
async def rate_message(request: RatingRequest):
    """ë©”ì‹œì§€ í‰ì  ì €ì¥"""
    rating_key = f"{request.message_id}_{request.user_id}"
    ratings_db[rating_key] = {
        "message_id": request.message_id,
        "user_id": request.user_id,
        "rating": request.rating,
        "feedback": request.feedback,
        "created_at": datetime.now().isoformat()
    }
    return {"success": True, "message": "Rating saved successfully"}

@app.get("/api/v1/messages/{message_id}/rating/{user_id}")
async def get_message_rating(message_id: str, user_id: str):
    """íŠ¹ì • ë©”ì‹œì§€ì˜ ì‚¬ìš©ì í‰ì  ì¡°íšŒ"""
    rating_key = f"{message_id}_{user_id}"
    if rating_key in ratings_db:
        return ratings_db[rating_key]
    else:
        return {"rating": None, "feedback": None}

# Document upload endpoints
@app.post("/api/v1/documents")
async def upload_document_default(
    file: UploadFile = File(...),
    user_id: str = Form(default="default_user")
):
    """ë¬¸ì„œ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸ - í”„ë¡ íŠ¸ì—”ë“œ ê¸°ë³¸ ê²½ë¡œ (Multi-format support with Docling)"""
    print(f"ğŸ“„ [UPLOAD] Document upload request: filename={file.filename}, user_id={user_id}")
    
    # íŒŒì¼ëª… ì¤‘ë³µ ê²€ì‚¬ (ë¨¼ì € í™•ì¸)
    if file.filename:
        duplicate_check = await check_duplicate_document(user_id, file.filename)
        if duplicate_check["duplicate_found"]:
            existing_doc = duplicate_check["existing_document"]
            print(f"âš ï¸ [UPLOAD] Duplicate file found: {file.filename}")
            return {
                "success": False,
                "error": "duplicate_file",
                "message": "ë™ì¼í•œ íŒŒì¼ëª…ì˜ ë¬¸ì„œê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.",
                "duplicate_info": {
                    "filename": existing_doc["filename"],
                    "upload_time": existing_doc["upload_time"],
                    "file_size": existing_doc["file_size"],
                    "processing_method": existing_doc["processing_method"]
                }
            }
    
    # ì‹¤ì œ íŒŒì¼ ë‚´ìš© ì½ê¸°
    file_content = await file.read()
    print(f"ğŸ“„ [UPLOAD] File size: {len(file_content)} bytes")
    
    # Mock document upload response
    doc_id = f"doc-{str(uuid.uuid4())[:8]}"
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    file_extension = file.filename.lower().split('.')[-1] if file.filename and '.' in file.filename else ""
    
    # Multi-format document processing
    processed_content = file_content
    processing_method = "basic"
    
    # Check if this is a structured document that needs Docling processing
    structured_formats = {'pdf', 'ppt', 'pptx', 'xlsx', 'xls', 'doc', 'docx'}
    
    if DOCLING_AVAILABLE and file_extension in structured_formats:
        try:
            print(f"ğŸ“„ [DOCLING] Processing {file_extension.upper()} document with Docling service")
            
            # Save file temporarily for processing
            temp_file_path = f"./uploads/temp_{doc_id}_{file.filename}"
            os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)
            
            with open(temp_file_path, 'wb') as temp_file:
                temp_file.write(file_content)
            
            # Process with Docling
            docling_client = DoclingClient()
            success, docling_result = await docling_client.convert_document(file_content, file.filename)
            
            # Use extracted text content
            if success and docling_result.get('content'):
                processed_content = docling_result['content'].encode('utf-8')
                processing_method = "docling"
                print(f"ğŸ“„ [DOCLING] Successfully processed document. Text length: {len(processed_content)} chars")
            else:
                print(f"âš ï¸ [DOCLING] No text content extracted, fallback to alternative processor")
                # Force fallback to alternative processor when no text extracted
                raise Exception("No text content from Docling")
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                
        except Exception as e:
            print(f"âš ï¸ [DOCLING] Failed to process document with Docling: {str(e)}")
            print(f"ğŸ“„ [FALLBACK] Trying alternative processor for {file.filename}")
            # Try alternative processor as fallback
            if ALT_PROCESSOR_AVAILABLE and file_extension in structured_formats:
                try:
                    alt_processor = AlternativeProcessor()
                    alt_success, alt_result = await alt_processor.process_document(file_content, file.filename)
                    
                    if alt_success and alt_result.get('content'):
                        processed_content = alt_result['content'].encode('utf-8')
                        processing_method = "alternative_processor"
                        print(f"ğŸ“„ [ALT-PROC] Successfully processed document. Text length: {len(processed_content)} chars")
                    else:
                        print(f"âš ï¸ [ALT-PROC] No text content extracted, using fallback message")
                        fallback_msg = f"ì´ ë¬¸ì„œ({file.filename})ëŠ” {file_extension.upper()} í˜•ì‹ì´ì§€ë§Œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”."
                        processed_content = fallback_msg.encode('utf-8')
                        processing_method = "text_fallback"
                except Exception as alt_e:
                    print(f"âš ï¸ [ALT-PROC] Alternative processor also failed: {str(alt_e)}")
                    processing_method = "basic_fallback"
            else:
                processing_method = "basic_fallback"
    
    elif file_extension in structured_formats and not DOCLING_AVAILABLE:
        print(f"âš ï¸ [DOCLING] Structured document detected ({file_extension}) but Docling not available")
        # Try alternative processor
        if ALT_PROCESSOR_AVAILABLE:
            try:
                print(f"ğŸ“„ [ALT-PROC] Processing {file_extension.upper()} document with alternative processor")
                
                # Save file temporarily for processing
                temp_file_path = f"./uploads/temp_{doc_id}_{file.filename}"
                os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)
                
                with open(temp_file_path, 'wb') as temp_file:
                    temp_file.write(file_content)
                
                # Process with Alternative Processor
                alt_processor = AlternativeProcessor()
                alt_success, alt_result = await alt_processor.process_document(file_content, file.filename)
                
                # Use extracted text content
                if alt_success and alt_result.get('content'):
                    processed_content = alt_result['content'].encode('utf-8')
                    processing_method = "alternative_processor"
                    print(f"ğŸ“„ [ALT-PROC] Successfully processed document. Text length: {len(processed_content)} chars")
                else:
                    print(f"âš ï¸ [ALT-PROC] No text content extracted, using fallback message")
                    fallback_msg = f"ì´ ë¬¸ì„œ({file.filename})ëŠ” {file_extension.upper()} í˜•ì‹ì´ì§€ë§Œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”."
                    processed_content = fallback_msg.encode('utf-8')
                    processing_method = "text_fallback"
                
                # Cleanup
                if os.path.exists(temp_file_path):
                    os.remove(temp_file_path)
                    
            except Exception as e:
                print(f"âš ï¸ [ALT-PROC] Failed to process document with alternative processor: {str(e)}")
                print(f"ğŸ“„ [WARNING] Document may not be processed optimally without document processing service")
        else:
            print(f"ğŸ“„ [WARNING] Document may not be processed optimally without document processing service")
    
    # ì‹¤ì œ RAG ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¬¸ì„œë¥¼ ì„ì‹œë¡œ ì €ì¥í•˜ê³  ì²˜ë¦¬
    temp_document = {
        "id": doc_id,
        "filename": file.filename,
        "content": processed_content,
        "user_id": user_id,
        "processed": True,
        "file_type": file_extension,
        "processing_method": processing_method,
        "upload_time": datetime.now().isoformat()
    }
    
    # ì „ì—­ documents ì €ì¥ì†Œì— ì¶”ê°€ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥)
    if user_id not in user_documents:
        user_documents[user_id] = []
    user_documents[user_id].append(temp_document)
    
    print(f"ğŸ“„ [UPLOAD] Document {doc_id} stored for user {user_id}")
    
    # Korean RAG ì„œë¹„ìŠ¤ë¡œ ìë™ ì „ì†¡í•˜ì—¬ ë²¡í„°í™” ì²˜ë¦¬
    rag_processing_status = "pending"
    if KOREAN_RAG_AVAILABLE:
        try:
            print(f"ğŸ‡°ğŸ‡· [RAG-AUTO] Sending document to Korean RAG service for vectorization")
            import httpx
            
            # ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ Korean RAG ì„œë¹„ìŠ¤ë¡œ ì „ì†¡
            text_content = processed_content.decode('utf-8') if isinstance(processed_content, bytes) else str(processed_content)
            
            rag_payload = {
                "title": file.filename,
                "content": text_content,
                "metadata": {
                    "filename": file.filename,
                    "file_size": len(processed_content),
                    "processing_method": processing_method,
                    "user_id": user_id,
                    "original_file_type": file_extension,
                    "upload_time": datetime.now().isoformat()
                },
                "document_id": doc_id
            }
            
            async with httpx.AsyncClient() as client:
                # Send to Korean RAG Orchestrator (Port 8008)
                orchestrator_payload = {
                    "user_id": user_id,
                    "filename": file.filename,
                    "content": text_content,
                    "metadata": {
                        "filename": file.filename,
                        "file_size": len(processed_content),
                        "processing_method": processing_method,
                        "original_file_type": file_extension,
                        "upload_time": datetime.now().isoformat(),
                        "document_id": doc_id
                    }
                }
                response = await client.post(
                    "http://localhost:8008/process_document",
                    json=orchestrator_payload,
                    timeout=30.0
                )
                if response.status_code == 200:
                    rag_response = await response.json()
                    if rag_response.get("status") == "success":
                        rag_processing_status = "vectorization_started"
                        print(f"âœ… [KOREAN-RAG] Document successfully sent to Korean RAG Orchestrator")
                        print(f"ğŸ”„ [KOREAN-RAG] Chunks processed: {rag_response.get('chunks_processed', 0)}")
                        print(f"ğŸ“Š [KOREAN-RAG] Chunks stored: {rag_response.get('chunks_stored', 0)}")
                    else:
                        print(f"âš ï¸ [KOREAN-RAG] Korean RAG Orchestrator returned error")
                else:
                    print(f"âš ï¸ [KOREAN-RAG] Failed to send to Korean RAG Orchestrator: HTTP {response.status_code}")
        except Exception as e:
            print(f"âŒ [RAG-AUTO] Error sending document to Korean RAG service: {e}")
            rag_processing_status = "rag_error"
    
    return {
        "success": True,
        "data": {
            "document_id": doc_id,
            "filename": file.filename,
            "message": "Document uploaded and processed successfully",
            "processing_status": "completed",
            "rag_processing": rag_processing_status,
            "vectorization_info": {
                "auto_sent_to_rag": KOREAN_RAG_AVAILABLE,
                "embedding_model": "jhgan/ko-sroberta-multitask" if KOREAN_RAG_AVAILABLE else None,
                "vector_db": "milvus" if KOREAN_RAG_AVAILABLE else None,
                "processing_method": processing_method
            }
        }
    }

@app.post("/api/v1/documents/upload")
async def upload_document(file: bytes = None, filename: str = "test.txt", user_id: str = "default_user"):
    """ë¬¸ì„œ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸ (Mock implementation)"""
    print(f"ğŸ“„ [UPLOAD] Document upload request: filename={filename}, user_id={user_id}")
    
    # Mock document upload response
    doc_id = f"doc-{str(uuid.uuid4())[:8]}"
    return {
        "success": True,
        "document_id": doc_id,
        "filename": filename,
        "message": "Document uploaded successfully (mock)",
        "processing_status": "queued"
    }

async def calculate_actual_chunk_count(user_id: str, document_id: str) -> int:
    """ì‹¤ì œ ì²­í¬ ê°œìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    print(f"ğŸ”¢ [CHUNK-COUNT] Calculating chunk count for {document_id}, user: {user_id}")
    try:
        # ê¸°ì¡´ ì²­í‚¹ ë¡œì§ì„ ì‚¬ìš©í•´ì„œ ì‹¤ì œ ì²­í¬ ê°œìˆ˜ ê³„ì‚°
        user_docs = user_documents.get(user_id, [])
        document = None
        
        for doc in user_docs:
            if doc["id"] == document_id:
                document = doc
                break
        
        if not document:
            return 1  # ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 1
        
        # ë¬¸ì„œ ë‚´ìš© ë””ì½”ë”©
        content = ""
        if isinstance(document["content"], bytes):
            try:
                content = document["content"].decode('utf-8')
            except UnicodeDecodeError:
                try:
                    content = document["content"].decode('cp949')
                except:
                    content = str(document["content"])
        else:
            content = str(document["content"])
        
        if not content.strip():
            print(f"ğŸ”¢ [CHUNK-COUNT] Content is empty, returning 1")
            return 1
        
        # ê¸°ì¡´ ì²­í‚¹ ë¡œì§ ì‚¬ìš© (ë¼ì¸ 2154-2276ê³¼ ë™ì¼í•œ ë¡œì§)
        chunks = []
        file_type = document.get("file_type", "").lower()
        is_structured_doc = file_type in ['pdf', 'pptx', 'docx', 'doc']
        print(f"ğŸ”¢ [CHUNK-COUNT] File type: {file_type}, is_structured_doc: {is_structured_doc}")
        
        if is_structured_doc:
            # êµ¬ì¡°í™”ëœ ë¬¸ì„œì˜ ì²­í‚¹ ë¡œì§
            major_sections = []
            for delimiter in ['\n\n\n', '\\n\\n', '\n\n', '\\n', '\n']:
                if delimiter in content:
                    major_sections = content.split(delimiter)
                    break
            
            if not major_sections or len(major_sections) == 1:
                major_sections = [content]
            
            MAX_CHUNK_SIZE = 1000
            MIN_CHUNK_SIZE = 100
            chunk_count = 0
            
            for section in major_sections:
                section = section.strip()
                if not section:
                    continue
                
                if len(section) <= MAX_CHUNK_SIZE:
                    if len(section) >= MIN_CHUNK_SIZE:
                        chunk_count += 1
                else:
                    # í° ì„¹ì…˜ì„ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
                    sentences = section.split('. ')
                    if len(sentences) == 1:
                        sentences = section.split('.\n')
                    if len(sentences) == 1:
                        sentences = section.split('\n')
                    
                    current_chunk = ""
                    for sentence in sentences:
                        if len(current_chunk + sentence) > MAX_CHUNK_SIZE and current_chunk.strip():
                            if len(current_chunk.strip()) >= MIN_CHUNK_SIZE:
                                chunk_count += 1
                            current_chunk = sentence
                        else:
                            current_chunk += (" " if current_chunk else "") + sentence
                    
                    if current_chunk.strip() and len(current_chunk.strip()) >= MIN_CHUNK_SIZE:
                        chunk_count += 1
            
            result = max(chunk_count, 1)  # ìµœì†Œ 1ê°œ ë³´ì¥
            print(f"ğŸ”¢ [CHUNK-COUNT] Structured document chunk count: {result}")
            return result
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¬¸ì„œëŠ” ê¸°ë³¸ ì²­í‚¹
            lines = content.split('\n')
            non_empty_lines = [line for line in lines if line.strip()]
            result = max(len(non_empty_lines) // 10, 1)
            print(f"ğŸ”¢ [CHUNK-COUNT] Text document chunk count: {result} (from {len(non_empty_lines)} non-empty lines)")
            return result
    
    except Exception as e:
        print(f"âŒ [CHUNK-COUNT] Error calculating chunk count for {document_id}: {str(e)}")
        return 1

@app.get("/api/v1/documents/{user_id}")
async def get_user_documents(user_id: str, limit: int = 20, offset: int = 0):
    """ì‚¬ìš©ì ë¬¸ì„œ ëª©ë¡ ë°˜í™˜ - Korean RAG Serviceì™€ ë¡œì»¬ ìŠ¤í† ì–´ì—ì„œ í†µí•© ì¡°íšŒ"""
    print(f"ğŸ“„ [DOCS] Getting documents for user: {user_id}")
    
    all_docs = []
    
    # 1. Korean RAG Serviceì—ì„œ ë¬¸ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    if KOREAN_RAG_AVAILABLE:
        try:
            print(f"ğŸ‡°ğŸ‡· [KOREAN-RAG] Fetching documents from Korean RAG service")
            import httpx
            
            async with httpx.AsyncClient() as client:
                response = await client.get("http://localhost:8008/documents")
                if response.status_code == 200:
                    rag_response = await response.json()
                    if rag_response.get("success") and "data" in rag_response:
                        rag_docs = rag_response["data"].get("documents", [])
                        print(f"ğŸ‡°ğŸ‡· [KOREAN-RAG] Found {len(rag_docs)} documents from Korean RAG service")
                        
                        # Korean RAG ë¬¸ì„œë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        for doc in rag_docs:
                            original_metadata = doc.get("metadata", {}).get("original_metadata", {})
                            chunk_count = doc.get("chunk_count", 1)
                            
                            # ì²­í‚¹, ì„ë² ë”©, ë²¡í„°í™” ìƒíƒœ ê³„ì‚°
                            chunking_status = "completed" if chunk_count > 0 else "pending"
                            embedding_status = "completed" if doc.get("embedding_count", 0) > 0 else chunking_status
                            vectorization_status = "completed" if doc.get("vector_count", 0) > 0 else embedding_status
                            
                            # ì²˜ë¦¬ ì§„í–‰ë¥  ê³„ì‚° (ê° ë‹¨ê³„ 33.33%)
                            progress = 0
                            if chunking_status == "completed": progress += 33.33
                            if embedding_status == "completed": progress += 33.33  
                            if vectorization_status == "completed": progress += 33.34
                            
                            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° (ìƒì„±ì¼ì ê¸°ì¤€)
                            from datetime import datetime
                            created_time = datetime.fromisoformat(doc.get("created_at", datetime.now().isoformat()).replace('Z', '+00:00'))
                            current_time = datetime.now()
                            time_elapsed = (current_time - created_time.replace(tzinfo=None)).total_seconds()
                            
                            # ê° ë‹¨ê³„ë³„ ì˜ˆìƒ ì‹œê°„ (ì²­í‚¹: 2ì´ˆ, ì„ë² ë”©: 5ì´ˆ, ë²¡í„°í™”: 3ì´ˆ)
                            chunking_time = 2 if chunking_status == "completed" else min(time_elapsed, 2)
                            embedding_time = 5 if embedding_status == "completed" else (min(time_elapsed - 2, 5) if time_elapsed > 2 else 0)
                            vectorization_time = 3 if vectorization_status == "completed" else (min(time_elapsed - 7, 3) if time_elapsed > 7 else 0)
                            
                            all_docs.append({
                                    "id": doc.get("document_id"),
                                    "filename": original_metadata.get("filename", "Unknown"),
                                    "title": original_metadata.get("title", doc.get("title", "ì œëª© ì—†ìŒ")),
                                    "created_at": doc.get("created_at", datetime.now().isoformat()),
                                    "file_size": original_metadata.get("file_size", 0),
                                    "is_processed": True,
                                    "chunk_count": chunk_count,
                                    "processing_method": original_metadata.get("processing_method", "korean_rag"),
                                    "source": "korean_rag",
                                    # ìƒˆë¡œ ì¶”ê°€ëœ ë²¡í„°í™” ìƒíƒœ ì •ë³´
                                    "processing_status": {
                                        "chunking": chunking_status,
                                        "embedding": embedding_status, 
                                        "vectorization": vectorization_status,
                                        "overall_progress": round(progress, 1),
                                        "embedding_model": "jhgan/ko-sroberta-multitask",
                                        "embedding_dimensions": 768,
                                        "vector_db": "milvus",
                                        "collection_name": "korean_documents",
                                        # ì²˜ë¦¬ ì‹œê°„ ì •ë³´ ì¶”ê°€
                                        "timing_info": {
                                            "total_elapsed_seconds": round(time_elapsed, 1),
                                            "chunking_time_seconds": round(chunking_time, 1),
                                            "embedding_time_seconds": round(embedding_time, 1),
                                            "vectorization_time_seconds": round(vectorization_time, 1),
                                            "created_at": doc.get("created_at"),
                                            "status_timestamps": {
                                                "upload_completed": doc.get("created_at"),
                                                "chunking_completed": doc.get("created_at") if chunking_status == "completed" else None,
                                                "embedding_completed": doc.get("created_at") if embedding_status == "completed" else None,
                                                "vectorization_completed": doc.get("created_at") if vectorization_status == "completed" else None
                                            }
                                        }
                                    },
                                    "rag_stats": {
                                        "chunk_count": chunk_count,
                                        "embedding_count": doc.get("embedding_count", 0),
                                        "vector_count": doc.get("vector_count", 0),
                                        "similarity_threshold": 0.3,
                                        "max_context_chunks": 5
                                    }
                                })
                else:
                    print(f"âš ï¸ [KOREAN-RAG] Failed to fetch documents: HTTP {response.status_code}")
        except Exception as e:
            print(f"âŒ [KOREAN-RAG] Error fetching documents from Korean RAG service: {e}")
    
    # 2. ë¡œì»¬ ìŠ¤í† ì–´ì—ì„œ ë¬¸ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    user_docs = user_documents.get(user_id, [])
    print(f"ğŸ“„ [DOCS] Found {len(user_docs)} local documents for user {user_id}")
    
    # ë¡œì»¬ ë¬¸ì„œë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    for doc in user_docs:
        file_size = len(doc["content"]) if isinstance(doc["content"], bytes) else len(str(doc["content"]))
        is_processed = doc.get("processed", True)
        processing_method = doc.get("processing_method", "local_storage")
        
        # ë¡œì»¬ ë¬¸ì„œì˜ ë²¡í„°í™” ìƒíƒœ (Korean RAG ì„œë¹„ìŠ¤ë¡œ ì „ì†¡ë˜ì§€ ì•Šì€ ìƒíƒœ)
        # ì—…ë¡œë“œ ì²˜ë¦¬ëŠ” ì™„ë£Œë˜ì—ˆì§€ë§Œ ë²¡í„°í™”ëŠ” ëŒ€ê¸° ì¤‘ì¸ ìƒíƒœ
        chunking_status = "pending"  # ë¡œì»¬ì—ì„œëŠ” ê¸°ë³¸ ì²­í‚¹ë§Œ ìˆ˜í–‰
        embedding_status = "pending"  # ì„ë² ë”© ë¯¸ì™„ë£Œ
        vectorization_status = "pending"  # Milvus ì €ì¥ ë¯¸ì™„ë£Œ
        overall_progress = 10.0  # ì—…ë¡œë“œë§Œ ì™„ë£Œëœ ìƒíƒœ
        
        if processing_method in ["docling", "alternative_processor"]:
            chunking_status = "completed"  # ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ
            overall_progress = 30.0
            
        all_docs.append({
            "id": doc["id"],
            "filename": doc["filename"],
            "title": doc["filename"],
            "created_at": doc.get("upload_time", "2024-01-01T00:00:00"),
            "file_size": file_size,
            "is_processed": is_processed,
            "chunk_count": await calculate_actual_chunk_count(user_id, doc["id"]),  # ì‹¤ì œ ì²­í¬ ìˆ˜ ê³„ì‚°
            "processing_method": processing_method,
            "source": "local",
            # ë²¡í„°í™” ìƒíƒœ ì •ë³´ (ë¡œì»¬ ë¬¸ì„œìš©)
            "processing_status": {
                "chunking": chunking_status,
                "embedding": embedding_status,
                "vectorization": vectorization_status, 
                "overall_progress": overall_progress,
                "embedding_model": "pending",
                "embedding_dimensions": 0,
                "vector_db": "not_stored",
                "collection_name": "none",
                "needs_rag_processing": True  # Korean RAG ì„œë¹„ìŠ¤ ì²˜ë¦¬ í•„ìš”
            },
            "rag_stats": {
                "chunk_count": await calculate_actual_chunk_count(user_id, doc["id"]),
                "embedding_count": 0,
                "vector_count": 0,
                "similarity_threshold": 0.0,
                "max_context_chunks": 0
            }
        })
    
    # 3. ë¬¸ì„œ IDë¡œ ì¤‘ë³µ ì œê±° (Korean RAGê°€ ìš°ì„ )
    unique_docs = {}
    for doc in all_docs:
        doc_id = doc["id"]
        if doc_id not in unique_docs or doc["source"] == "korean_rag":
            unique_docs[doc_id] = doc
    
    final_docs = list(unique_docs.values())
    
    # 4. ìƒì„± ì‹œê°„ìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    final_docs.sort(key=lambda x: x["created_at"], reverse=True)
    
    # 5. í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
    total = len(final_docs)
    paginated_docs = final_docs[offset:offset + limit]
    
    print(f"ğŸ“„ [DOCS] Returning {len(paginated_docs)} documents (total: {total}) - {sum(1 for d in final_docs if d['source'] == 'korean_rag')} from Korean RAG, {sum(1 for d in final_docs if d['source'] == 'local')} local")
    
    return {
        "documents": paginated_docs,
        "total": total,
        "limit": limit,
        "offset": offset
    }

# Document content and duplicate checking endpoints
@app.get("/api/v1/documents/{user_id}/{document_id}/content")
async def get_document_content(user_id: str, document_id: str):
    """ë¬¸ì„œ ë‚´ìš© ìƒì„¸ ì¡°íšŒ - ë¬¸ì„œ ë·°ì–´ìš©"""
    print(f"ğŸ“„ [DOC-CONTENT] Getting content for document: {document_id}, user: {user_id}")
    
    try:
        # ë¡œì»¬ ë¬¸ì„œ ìŠ¤í† ì–´ì—ì„œ ì¡°íšŒ
        user_docs = user_documents.get(user_id, [])
        document = None
        
        for doc in user_docs:
            if doc["id"] == document_id:
                document = doc
                break
        
        if not document:
            # Korean RAG Serviceì—ì„œë„ ì¡°íšŒ ì‹œë„
            if KOREAN_RAG_AVAILABLE:
                try:
                    import httpx
                    async with httpx.AsyncClient() as client:
                        response = await client.get(f"http://localhost:8008/documents/{document_id}")
                        if response.status_code == 200:
                                rag_doc = await response.json()
                                if rag_doc.get("success") and rag_doc.get("document"):
                                    doc_data = rag_doc["document"]
                                    return {
                                        "success": True,
                                        "document": {
                                            "id": doc_data.get("id"),
                                            "filename": doc_data.get("filename"),
                                            "title": doc_data.get("title", doc_data.get("filename")),
                                            "content": doc_data.get("content", ""),
                                            "file_size": doc_data.get("file_size", 0),
                                            "created_at": doc_data.get("created_at"),
                                            "processing_method": doc_data.get("processing_method", "korean_rag"),
                                            "chunk_count": doc_data.get("chunk_count", 0)
                                        }
                                    }
                except Exception as e:
                    print(f"âš ï¸ [DOC-CONTENT] Korean RAG service error: {str(e)}")
            
            return {
                "success": False,
                "error": "Document not found",
                "message": "ìš”ì²­í•˜ì‹  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ë¬¸ì„œ ë‚´ìš© ë””ì½”ë”©
        content = ""
        if isinstance(document["content"], bytes):
            try:
                content = document["content"].decode('utf-8')
            except UnicodeDecodeError:
                try:
                    content = document["content"].decode('cp949')
                except:
                    content = str(document["content"])
        else:
            content = str(document["content"])
        
        print(f"ğŸ“„ [DOC-CONTENT] Found document: {document['filename']}, content length: {len(content)} chars")
        
        return {
            "success": True,
            "document": {
                "id": document["id"],
                "filename": document["filename"],
                "title": document["filename"],
                "content": content,
                "file_size": len(document["content"]) if isinstance(document["content"], bytes) else len(content),
                "created_at": document.get("upload_time", datetime.now().isoformat()),
                "processing_method": document.get("processing_method", "basic"),
                "file_type": document.get("file_type", "unknown")
            }
        }
        
    except Exception as e:
        print(f"âŒ [DOC-CONTENT] Error getting document content: {str(e)}")
        return {
            "success": False,
            "error": "Internal server error",
            "message": "ë¬¸ì„œ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }

@app.get("/api/v1/documents/{user_id}/check-duplicate")
async def check_duplicate_document(user_id: str, filename: str):
    """íŒŒì¼ëª… ì¤‘ë³µ ê²€ì‚¬"""
    print(f"ğŸ“„ [DUPLICATE-CHECK] Checking for duplicate: {filename}, user: {user_id}")
    
    try:
        # ë¡œì»¬ ë¬¸ì„œ ìŠ¤í† ì–´ì—ì„œ ì¤‘ë³µ ê²€ì‚¬
        user_docs = user_documents.get(user_id, [])
        duplicate_found = False
        existing_doc = None
        
        for doc in user_docs:
            if doc["filename"].lower() == filename.lower():
                duplicate_found = True
                existing_doc = {
                    "id": doc["id"],
                    "filename": doc["filename"],
                    "upload_time": doc.get("upload_time"),
                    "file_size": len(doc["content"]) if isinstance(doc["content"], bytes) else len(str(doc["content"])),
                    "processing_method": doc.get("processing_method", "basic")
                }
                break
        
        # Korean RAG Serviceì—ì„œë„ ì¤‘ë³µ ê²€ì‚¬
        if not duplicate_found and KOREAN_RAG_AVAILABLE:
            try:
                import httpx
                async with httpx.AsyncClient() as client:
                    response = await client.get(f"http://localhost:8008/documents/{user_id}")
                    if response.status_code == 200:
                            rag_response = await response.json()
                            if rag_response.get("documents"):
                                for rag_doc in rag_response["documents"]:
                                    if rag_doc.get("filename", "").lower() == filename.lower():
                                        duplicate_found = True
                                        existing_doc = {
                                            "id": rag_doc.get("id"),
                                            "filename": rag_doc.get("filename"),
                                            "upload_time": rag_doc.get("created_at"),
                                            "file_size": rag_doc.get("file_size", 0),
                                            "processing_method": rag_doc.get("processing_method", "korean_rag")
                                        }
                                        break
            except Exception as e:
                print(f"âš ï¸ [DUPLICATE-CHECK] Korean RAG service error: {str(e)}")
        
        result = {
            "filename": filename,
            "duplicate_found": duplicate_found,
            "existing_document": existing_doc if duplicate_found else None
        }
        
        print(f"ğŸ“„ [DUPLICATE-CHECK] Result: {'DUPLICATE FOUND' if duplicate_found else 'NO DUPLICATE'}")
        return result
        
    except Exception as e:
        print(f"âŒ [DUPLICATE-CHECK] Error checking duplicate: {str(e)}")
        return {
            "filename": filename,
            "duplicate_found": False,
            "error": str(e)
        }

@app.get("/api/v1/search/web/engines")
async def get_search_engines():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ì—”ì§„ ëª©ë¡ ë°˜í™˜"""
    try:
        engines = [
            {
                "id": "google",
                "name": "Google",
                "description": "ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê²€ìƒ‰ì—”ì§„",
                "available": True
            },
            {
                "id": "bing",
                "name": "Bing",
                "description": "Microsoftì˜ ê²€ìƒ‰ì—”ì§„",
                "available": True
            },
            {
                "id": "duckduckgo",
                "name": "DuckDuckGo",
                "description": "ê°œì¸ì •ë³´ ë³´í˜¸ ì¤‘ì‹¬ ê²€ìƒ‰ì—”ì§„",
                "available": True
            },
            {
                "id": "wikipedia",
                "name": "Wikipedia",
                "description": "ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰",
                "available": True
            }
        ]
        
        print(f"ğŸ” [SEARCH-ENGINES] Returning {len(engines)} available search engines")
        return {"engines": engines}
        
    except Exception as e:
        print(f"âŒ [SEARCH-ENGINES] Error getting search engines: {str(e)}")
        return {"engines": []}

@app.get("/api/v1/documents/{user_id}/{document_id}/chunks")
async def get_document_chunks(user_id: str, document_id: str):
    """ë¬¸ì„œì˜ ì²­í‚¹ëœ í…ìŠ¤íŠ¸ ì¡°íšŒ - ì²­í¬ ë·°ì–´ìš©"""
    print(f"ğŸ“„ [DOC-CHUNKS] Getting chunks for document: {document_id}, user: {user_id}")
    
    try:
        # Korean RAG ë¬¸ì„œì¸ì§€ í™•ì¸ (doc_ë¡œ ì‹œì‘)
        if document_id.startswith('doc_'):
            # Korean RAG Serviceì—ì„œ ì‹¤ì œ ì²­í¬ ë‚´ìš© ì¡°íšŒ
            if KOREAN_RAG_AVAILABLE:
                try:
                    import httpx
                    async with httpx.AsyncClient() as client:
                        # Korean RAG Serviceì˜ ìƒˆë¡œìš´ chunks APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì²­í¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                        response = await client.get(f"http://localhost:8008/documents/{document_id}/chunks")
                        if response.status_code == 200:
                                chunks_result = await response.json()
                                if chunks_result.get("success") and chunks_result.get("data"):
                                    chunks_data = chunks_result["data"]
                                    raw_chunks = chunks_data.get("chunks", [])
                                    
                                    # ì‹¤ì œ ì²­í¬ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                    document_chunks = []
                                    for i, chunk in enumerate(raw_chunks):
                                        document_chunks.append({
                                            "chunk_id": chunk.get("id", f"chunk_{i}"),
                                            "text": chunk.get("text", ""),
                                            "chunk_index": chunk.get("chunk_id", i),
                                            "similarity_score": 1.0,  # ì›ë³¸ ì²­í¬ì´ë¯€ë¡œ ì™„ë²½í•œ ì¼ì¹˜
                                            "metadata": chunk.get("metadata", {}),
                                            "length": len(chunk.get("text", ""))
                                        })
                                    
                                    if document_chunks:
                                        print(f"âœ… [DOC-CHUNKS] Retrieved {len(document_chunks)} actual chunks from Korean RAG")
                                        return {
                                            "success": True,
                                            "document_id": document_id,
                                            "total_chunks": len(document_chunks),
                                            "chunks": document_chunks,
                                            "source": "korean_rag_actual",
                                            "message": f"Korean RAG ë¬¸ì„œì˜ ì‹¤ì œ {len(document_chunks)}ê°œ ì²­í¬ë¥¼ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤."
                                        }
                        
                        # ì²­í¬ APIê°€ ì‹¤íŒ¨í•œ ê²½ìš°, ë¬¸ì„œ ì •ë³´ë§Œ ë°˜í™˜ (fallback)
                        print(f"âš ï¸ [DOC-CHUNKS] Korean RAG chunks API failed, falling back to placeholder")
                        async with session.get(f"http://localhost:8008/documents") as response:
                            if response.status == 200:
                                docs_result = await response.json()
                                if docs_result.get("success"):
                                    documents = docs_result["data"].get("documents", [])
                                    for doc in documents:
                                        if doc.get("document_id") == document_id:
                                            chunk_count = doc.get("chunk_count", 0)
                                            # ì²­í¬ ê°œìˆ˜ë§Œí¼ ë”ë¯¸ ì²­í¬ ìƒì„±
                                            dummy_chunks = []
                                            for i in range(chunk_count):
                                                dummy_chunks.append({
                                                    "chunk_id": f"{document_id}_chunk_{i}",
                                                    "text": f"[ì²­í¬ {i+1}] ì´ ì²­í¬ì˜ ì‹¤ì œ ë‚´ìš©ì€ Korean RAG ì‹œìŠ¤í…œì˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                                                    "chunk_index": i,
                                                    "similarity_score": 0.0,
                                                    "metadata": {"document_id": document_id, "chunk_type": "placeholder"},
                                                    "length": 50
                                                })
                                            
                                            return {
                                                "success": True,
                                                "document_id": document_id,
                                                "total_chunks": chunk_count,
                                                "chunks": dummy_chunks,
                                                "source": "korean_rag_placeholder",
                                                "message": f"Korean RAG ë¬¸ì„œì˜ {chunk_count}ê°œ ì²­í¬ ì •ë³´ë¥¼ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤. (ì‹¤ì œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° DBì— ì €ì¥)"
                                            }
                except Exception as e:
                    print(f"âš ï¸ [DOC-CHUNKS] Korean RAG service error: {str(e)}")
        
        # ì¼ë°˜ ë¬¸ì„œ ì²˜ë¦¬
        user_docs = user_documents.get(user_id, [])
        document = None
        
        for doc in user_docs:
            if doc["id"] == document_id:
                document = doc
                break
        
        if not document:
            return {
                "success": False,
                "error": "Document not found",
                "message": "ìš”ì²­í•˜ì‹  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ë¬¸ì„œ ë‚´ìš© ë””ì½”ë”©
        content = ""
        if isinstance(document["content"], bytes):
            try:
                content = document["content"].decode('utf-8')
            except UnicodeDecodeError:
                try:
                    content = document["content"].decode('cp949')
                except:
                    content = str(document["content"])
        else:
            content = str(document["content"])
        
        # ê°œì„ ëœ ì²­í‚¹ ë¡œì§ - ë‹¤ì–‘í•œ ë¶„í•  ê¸°ì¤€ ì‚¬ìš©
        chunks = []
        
        # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ì²­í‚¹ ì „ëµ
        file_type = document.get("file_type", "").lower()
        is_structured_doc = file_type in ['pdf', 'pptx', 'docx', 'doc']
        
        if is_structured_doc:
            # êµ¬ì¡°í™”ëœ ë¬¸ì„œ (PDF, PPT, Word)ì˜ ê²½ìš° ë” ì„¸ë¶„í™”ëœ ì²­í‚¹
            # 1. ë¨¼ì € í° ì„¹ì…˜ìœ¼ë¡œ ë¶„í•  (ì—¬ëŸ¬ ê°œí–‰, í˜ì´ì§€ êµ¬ë¶„ì ë“±)
            major_sections = []
            for delimiter in ['\n\n\n', '\\n\\n', '\n\n', '\\n', '\n']:
                if delimiter in content:
                    major_sections = content.split(delimiter)
                    break
            
            if not major_sections or len(major_sections) == 1:
                major_sections = [content]
            
            # 2. ê° ì„¹ì…˜ì„ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• 
            MAX_CHUNK_SIZE = 1000  # ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
            MIN_CHUNK_SIZE = 100   # ìµœì†Œ ì²­í¬ í¬ê¸°
            
            chunk_idx = 0
            for section_idx, section in enumerate(major_sections):
                section = section.strip()
                if not section:
                    continue
                
                if len(section) <= MAX_CHUNK_SIZE:
                    # ì„¹ì…˜ì´ ì ì ˆí•œ í¬ê¸°ë©´ ê·¸ëŒ€ë¡œ ì²­í¬ë¡œ ì‚¬ìš©
                    if len(section) >= MIN_CHUNK_SIZE:
                        chunks.append({
                            "chunk_id": f"{document_id}_section_{chunk_idx}",
                            "text": section,
                            "chunk_index": chunk_idx,
                            "similarity_score": 1.0,
                            "metadata": {
                                "document_id": document_id,
                                "chunk_type": "section",
                                "section_number": section_idx + 1,
                                "file_type": file_type
                            },
                            "length": len(section)
                        })
                        chunk_idx += 1
                else:
                    # ë„ˆë¬´ ê¸´ ì„¹ì…˜ì€ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• 
                    sentences = []
                    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„
                    for sent_delimiter in ['. ', '.\n', '! ', '?\n', '? ']:
                        if sent_delimiter in section:
                            sentences = section.split(sent_delimiter)
                            break
                    
                    if not sentences:
                        # ë¬¸ì¥ ë¶„í• ì´ ì•ˆ ë˜ë©´ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• 
                        sentences = section.split('\n')
                    
                    current_chunk = ""
                    for sentence in sentences:
                        sentence = sentence.strip()
                        if not sentence:
                            continue
                            
                        # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° í™•ì¸
                        potential_chunk = current_chunk + (" " if current_chunk else "") + sentence
                        
                        if len(potential_chunk) <= MAX_CHUNK_SIZE:
                            current_chunk = potential_chunk
                        else:
                            # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                            if current_chunk and len(current_chunk) >= MIN_CHUNK_SIZE:
                                chunks.append({
                                    "chunk_id": f"{document_id}_chunk_{chunk_idx}",
                                    "text": current_chunk,
                                    "chunk_index": chunk_idx,
                                    "similarity_score": 1.0,
                                    "metadata": {
                                        "document_id": document_id,
                                        "chunk_type": "smart_chunk",
                                        "section_number": section_idx + 1,
                                        "file_type": file_type
                                    },
                                    "length": len(current_chunk)
                                })
                                chunk_idx += 1
                            current_chunk = sentence
                    
                    # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
                    if current_chunk and len(current_chunk) >= MIN_CHUNK_SIZE:
                        chunks.append({
                            "chunk_id": f"{document_id}_final_chunk_{chunk_idx}",
                            "text": current_chunk,
                            "chunk_index": chunk_idx,
                            "similarity_score": 1.0,
                            "metadata": {
                                "document_id": document_id,
                                "chunk_type": "smart_chunk",
                                "section_number": section_idx + 1,
                                "file_type": file_type
                            },
                            "length": len(current_chunk)
                        })
                        chunk_idx += 1
        else:
            # í…ìŠ¤íŠ¸ íŒŒì¼ ë“± ê¸°ë³¸ ë¬¸ì„œëŠ” ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            paragraphs = content.split('\n\n')
            
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():  # ë¹ˆ ë¬¸ë‹¨ ì œì™¸
                    chunks.append({
                        "chunk_id": f"{document_id}_paragraph_{i}",
                        "text": paragraph.strip(),
                        "chunk_index": i,
                        "similarity_score": 1.0,  # ì¼ë°˜ ë¬¸ì„œëŠ” ëª¨ë“  ì²­í¬ê°€ ê´€ë ¨ì„± 100%
                        "metadata": {
                            "document_id": document_id,
                            "chunk_type": "paragraph",
                            "paragraph_number": i + 1
                        },
                        "length": len(paragraph.strip())
                    })
        
        # ì²­í¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬
        if not chunks:
            chunks.append({
                "chunk_id": f"{document_id}_full",
                "text": content,
                "chunk_index": 0,
                "similarity_score": 1.0,
                "metadata": {
                    "document_id": document_id,
                    "chunk_type": "full_document"
                },
                "length": len(content)
            })
        
        print(f"ğŸ“„ [DOC-CHUNKS] Found {len(chunks)} chunks for document: {document['filename']}")
        
        return {
            "success": True,
            "document_id": document_id,
            "total_chunks": len(chunks),
            "chunks": chunks,
            "source": "local",
            "message": f"ì¼ë°˜ ë¬¸ì„œì˜ {len(chunks)}ê°œ ì²­í¬ë¥¼ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        print(f"âŒ [DOC-CHUNKS] Error getting document chunks: {str(e)}")
        return {
            "success": False,
            "error": "Internal server error",
            "message": "ë¬¸ì„œ ì²­í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }

@app.get("/health")
async def health_check():
    """Health check"""
    return {"status": "ok"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)