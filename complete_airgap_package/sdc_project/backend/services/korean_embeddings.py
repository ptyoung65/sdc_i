"""
Korean Embeddings Service with Docling Integration
í•œêµ­ì–´ ì„ë² ë”© ì„œë¹„ìŠ¤ - Doclingê³¼ TF-IDFë¥¼ í™œìš©í•œ í•œêµ­ì–´ íŠ¹í™” ë²¡í„°í™”
PyTorch/Transformers ì˜ì¡´ì„± ì—†ì´ êµ¬í˜„
"""

import logging
import numpy as np
from typing import List, Dict, Any, Optional, Union, Tuple
from pathlib import Path
import pickle
import hashlib
import json
import re
from collections import Counter
import math
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import requests
from datetime import datetime
import os

# Korean text processing (using kiwipiepy if available)
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False
    logging.warning("Kiwi not available, using basic tokenization")

logger = logging.getLogger(__name__)

class KoreanTextProcessor:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.kiwi = None
        if KIWI_AVAILABLE:
            try:
                self.kiwi = Kiwi()
                logger.info("âœ… Kiwi í•œêµ­ì–´ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                
    def preprocess_text(self, text: str) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ê¸°ë³¸ì ì¸ ì •ë¦¬
        text = text.strip()
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€
        text = re.sub(r'[^\w\s.!?ê°€-í£]', ' ', text)
        
        return text.strip()
    
    def extract_keywords(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        tokens = self.tokenize(text)
        
        # ê¸¸ì´ê°€ 2ì ì´ìƒì¸ í† í°ë§Œ í‚¤ì›Œë“œë¡œ ê°„ì£¼
        keywords = [token for token in tokens if len(token) >= 2]
        
        # ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        from collections import Counter
        counter = Counter(keywords)
        return [word for word, count in counter.most_common(10)]
    
    def tokenize(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ í† í°í™”"""
        if self.kiwi:
            try:
                # Kiwië¥¼ ì‚¬ìš©í•œ í˜•íƒœì†Œ ë¶„ì„
                result = self.kiwi.tokenize(text)
                tokens = []
                for token in result:
                    # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ (ì˜ë¯¸ìˆëŠ” í† í°)
                    if token.tag in ['NNG', 'NNP', 'VV', 'VA', 'SL']:
                        tokens.append(token.form)
                return tokens
            except Exception as e:
                logger.warning(f"Kiwi tokenization failed: {e}")
                
        # Fallback: ê³µë°± ê¸°ë°˜ ë¶„ë¦¬ + í•œê¸€ ì¶”ì¶œ
        tokens = []
        words = text.split()
        for word in words:
            # í•œê¸€ë§Œ ì¶”ì¶œ
            korean_chars = re.findall(r'[ê°€-í£]+', word)
            tokens.extend(korean_chars)
        return tokens
    
    def chunk_text(self, 
                   text: str, 
                   chunk_size: int = 500,
                   overlap: int = 50) -> List[str]:
        """
        í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹
        
        Args:
            text: ì²­í‚¹í•  í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
            chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ í¬ê¸°
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í•œêµ­ì–´ ë¬¸ì¥ ë¶€í˜¸ ê³ ë ¤)
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text)
        
        current_chunk = ""
        chunk_index = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
            if len(current_chunk) + len(sentence) <= chunk_size:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk)
                    chunk_index += 1
                
                # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© í¬í•¨)
                if overlap > 0 and current_chunk:
                    # ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ í¬í•¨
                    overlap_text = current_chunk[-overlap:]
                    current_chunk = overlap_text + " " + sentence
                else:
                    current_chunk = sentence
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

class DoclingClient:
    """Docling ì„œë¹„ìŠ¤ì™€ í†µì‹ í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, host: str = None, port: int = None):
        self.host = host or os.getenv("DOCLING_HOST", "localhost")
        self.port = port or int(os.getenv("DOCLING_PORT", "8501"))
        self.base_url = f"http://{self.host}:{self.port}"
        
    def process_document(self, file_path: str) -> Dict[str, Any]:
        """
        Doclingì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì²˜ë¦¬
        
        Args:
            file_path: ì²˜ë¦¬í•  ë¬¸ì„œ ê²½ë¡œ
            
        Returns:
            ì²˜ë¦¬ëœ ë¬¸ì„œ ë°ì´í„°
        """
        try:
            # Docling API í˜¸ì¶œ (ì‹¤ì œ APIì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
            with open(file_path, 'rb') as f:
                files = {'file': f}
                response = requests.post(
                    f"{self.base_url}/process",
                    files=files,
                    timeout=30
                )
                
            if response.status_code == 200:
                return response.json()
            else:
                logger.warning(f"Docling processing failed: {response.status_code}")
                return None
                
        except requests.exceptions.RequestException as e:
            logger.warning(f"Docling service unavailable: {e}")
            return None
        except Exception as e:
            logger.error(f"Docling processing error: {e}")
            return None

class KoreanEmbeddingService:
    """
    í•œêµ­ì–´ ì„ë² ë”© ì„œë¹„ìŠ¤ - TF-IDF ê¸°ë°˜ ë²¡í„°í™”
    PyTorch/Transformers ì—†ì´ scikit-learn ì‚¬ìš©
    """
    
    def __init__(self, embedding_dim: int = 768):
        """
        í•œêµ­ì–´ ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            embedding_dim: ì„ë² ë”© ì°¨ì› (TF-IDF íŠ¹ì§• ìˆ˜)
        """
        self.embedding_dim = embedding_dim
        self.text_processor = KoreanTextProcessor()
        self.docling_client = DoclingClient()
        
        # TF-IDF ë²¡í„°ë¼ì´ì € (ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •)
        self.vectorizer = TfidfVectorizer(
            max_features=embedding_dim,
            tokenizer=self.text_processor.tokenize,
            token_pattern=None,  # Custom tokenizer ì‚¬ìš©
            ngram_range=(1, 2),  # Unigram + Bigram
            min_df=1,
            max_df=1.0,  # ë‹¨ì¼ ë¬¸ì„œì—ì„œë„ ì‘ë™í•˜ë„ë¡ 1.0ìœ¼ë¡œ ì„¤ì •
            sublinear_tf=True,  # log(tf) ì‚¬ìš©
            use_idf=True
        )
        
        # ë¬¸ì„œ ì½”í¼ìŠ¤ (TF-IDF í•™ìŠµìš©)
        self.corpus = []
        self.is_fitted = False
        
        # ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = Path("./vector_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        # í•œêµ­ì–´ íŠ¹í™” ê°€ì¤‘ì¹˜
        self.korean_weights = self._initialize_korean_weights()
        
        logger.info(f"âœ… í•œêµ­ì–´ ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì°¨ì›: {embedding_dim})")
    
    def _initialize_korean_weights(self) -> Dict[str, float]:
        """í•œêµ­ì–´ íŠ¹í™” ë‹¨ì–´ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        # ì¤‘ìš” í•œêµ­ì–´ í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
        return {
            # ê¸°ìˆ  ìš©ì–´
            "ì¸ê³µì§€ëŠ¥": 2.0, "ë¨¸ì‹ ëŸ¬ë‹": 2.0, "ë”¥ëŸ¬ë‹": 2.0,
            "ë°ì´í„°": 1.5, "ë¶„ì„": 1.5, "ëª¨ë¸": 1.5,
            "ì•Œê³ ë¦¬ì¦˜": 1.8, "í•™ìŠµ": 1.5, "ì˜ˆì¸¡": 1.5,
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´
            "ê²½ì˜": 1.5, "ì „ëµ": 1.5, "ì‹œì¥": 1.5,
            "ê³ ê°": 1.8, "ì„œë¹„ìŠ¤": 1.5, "í’ˆì§ˆ": 1.5,
            
            # ì¼ë°˜ ì¤‘ìš” ë‹¨ì–´
            "ì¤‘ìš”": 1.3, "í•„ìˆ˜": 1.3, "í•µì‹¬": 1.5,
            "ì£¼ìš”": 1.3, "ê¸°ë³¸": 1.2, "í•„ìš”": 1.2
        }
    
    def _get_cache_key(self, text: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(f"tfidf:{text}".encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[np.ndarray]:
        """ìºì‹œì—ì„œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°"""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    def _save_to_cache(self, cache_key: str, embedding: np.ndarray):
        """ì„ë² ë”©ì„ ìºì‹œì— ì €ì¥"""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(embedding, f)
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _apply_korean_weights(self, text: str, vector: np.ndarray) -> np.ndarray:
        """í•œêµ­ì–´ ê°€ì¤‘ì¹˜ ì ìš©"""
        # í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë‹¨ì–´ ì°¾ê¸°
        weight_multiplier = 1.0
        for keyword, weight in self.korean_weights.items():
            if keyword in text:
                weight_multiplier = max(weight_multiplier, weight)
        
        # ë²¡í„°ì— ê°€ì¤‘ì¹˜ ì ìš©
        weighted_vector = vector * weight_multiplier
        
        # ì •ê·œí™”
        norm = np.linalg.norm(weighted_vector)
        if norm > 0:
            weighted_vector = weighted_vector / norm
            
        return weighted_vector
    
    def embed_text(self, text: str) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            text: ë²¡í„°í™”í•  í…ìŠ¤íŠ¸
            
        Returns:
            TF-IDF ë²¡í„° (numpy array)
        """
        if not text.strip():
            return np.zeros(self.embedding_dim)
        
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(text)
        cached_embedding = self._get_from_cache(cache_key)
        if cached_embedding is not None:
            return cached_embedding
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_text = self.text_processor.preprocess_text(text)
        
        # TF-IDF ë²¡í„°í™”
        try:
            # ë²¡í„°ë¼ì´ì €ê°€ ì•„ì§ fitë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì´ í…ìŠ¤íŠ¸ë¡œ fit
            if not hasattr(self.vectorizer, 'vocabulary_') or self.vectorizer.vocabulary_ is None:
                self.vectorizer.fit([processed_text])
            
            # ë²¡í„° ë³€í™˜
            vector = self.vectorizer.transform([processed_text])
            vector_array = vector.toarray()[0]
            
            # ì°¨ì› ì¡°ì •
            if len(vector_array) < self.embedding_dim:
                # ë¶€ì¡±í•œ ì°¨ì›ì€ 0ìœ¼ë¡œ íŒ¨ë”©
                padded_vector = np.zeros(self.embedding_dim)
                padded_vector[:len(vector_array)] = vector_array
                vector_array = padded_vector
            elif len(vector_array) > self.embedding_dim:
                # ì´ˆê³¼ ì°¨ì›ì€ ì˜ë¼ë‚´ê¸°
                vector_array = vector_array[:self.embedding_dim]
            
            # í•œêµ­ì–´ ê°€ì¤‘ì¹˜ ì ìš©
            weighted_vector = self._apply_korean_weights(text, vector_array)
            
            # ìºì‹œì— ì €ì¥
            self._save_to_cache(cache_key, weighted_vector)
            
            return weighted_vector
            
        except Exception as e:
            logger.error(f"TF-IDF ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ëœë¤ ë²¡í„° ë°˜í™˜
            return np.random.rand(self.embedding_dim) * 0.1
    
    def fit_corpus(self, texts: List[str]):
        """
        ì½”í¼ìŠ¤ë¡œ TF-IDF ëª¨ë¸ í•™ìŠµ
        
        Args:
            texts: í•™ìŠµí•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return
            
        logger.info(f"ğŸ“š TF-IDF ëª¨ë¸ í•™ìŠµ ì¤‘ ({len(texts)}ê°œ ë¬¸ì„œ)...")
        
        # ì½”í¼ìŠ¤ ì—…ë°ì´íŠ¸
        self.corpus.extend(texts)
        
        # TF-IDF í•™ìŠµ
        try:
            self.vectorizer.fit(self.corpus)
            self.is_fitted = True
            logger.info("âœ… TF-IDF ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"TF-IDF í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    def encode_single(self, text: str, use_cache: bool = True) -> np.ndarray:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        if not text or not text.strip():
            return np.zeros(self.embedding_dim)
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            cache_key = self._get_cache_key(text)
            cached_embedding = self._get_from_cache(cache_key)
            if cached_embedding is not None:
                return cached_embedding
        
        try:
            # TF-IDF ë²¡í„° ìƒì„±
            if not self.is_fitted:
                # ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì€ ê²½ìš°, ë‹¨ì¼ ë¬¸ì„œë¡œ ì„ì‹œ í•™ìŠµ
                self.fit_corpus([text])
            
            # TF-IDF ë³€í™˜
            tfidf_vector = self.vectorizer.transform([text]).toarray()[0]
            
            # ì°¨ì› ì¡°ì • (í•„ìš”í•œ ê²½ìš°)
            if len(tfidf_vector) < self.embedding_dim:
                # íŒ¨ë”©
                embedding = np.pad(tfidf_vector, 
                                 (0, self.embedding_dim - len(tfidf_vector)), 
                                 mode='constant')
            else:
                embedding = tfidf_vector[:self.embedding_dim]
            
            # í•œêµ­ì–´ ê°€ì¤‘ì¹˜ ì ìš©
            embedding = self._apply_korean_weights(text, embedding)
            
            # ìºì‹œ ì €ì¥
            if use_cache:
                self._save_to_cache(cache_key, embedding)
            
            return embedding
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros(self.embedding_dim)
    
    def encode_batch(self, 
                    texts: List[str], 
                    batch_size: int = 32,
                    use_cache: bool = True) -> List[np.ndarray]:
        """
        ë°°ì¹˜ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return []
        
        embeddings = []
        
        for text in texts:
            embedding = self.encode_single(text, use_cache=use_cache)
            embeddings.append(embedding)
        
        return embeddings
    
    def process_document_with_docling(self, 
                                     file_path: str,
                                     chunk_size: int = 500) -> List[Dict[str, Any]]:
        """
        Doclingì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹
        
        Args:
            file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            chunk_size: ì²­í¬ í¬ê¸°
            
        Returns:
            ì²˜ë¦¬ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        
        # Doclingìœ¼ë¡œ ë¬¸ì„œ ì²˜ë¦¬ ì‹œë„
        docling_result = self.docling_client.process_document(file_path)
        
        if docling_result and 'text' in docling_result:
            # Docling ì„±ê³µ
            text = docling_result['text']
            metadata = docling_result.get('metadata', {})
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹
            text_chunks = self.text_processor.chunk_text(text, chunk_size=chunk_size)
            
            for chunk in text_chunks:
                # ê° ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±
                embedding = self.encode_single(chunk['text'])
                
                chunks.append({
                    'text': chunk['text'],
                    'embedding': embedding.tolist(),
                    'metadata': {
                        **metadata,
                        'chunk_index': chunk['index'],
                        'start_char': chunk['start_char'],
                        'processing_method': 'docling'
                    }
                })
        else:
            # Docling ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            logger.info("Docling ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‚¬ìš©")
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                # í…ìŠ¤íŠ¸ ì²­í‚¹
                text_chunks = self.text_processor.chunk_text(text, chunk_size=chunk_size)
                
                for chunk in text_chunks:
                    embedding = self.encode_single(chunk['text'])
                    
                    chunks.append({
                        'text': chunk['text'],
                        'embedding': embedding.tolist(),
                        'metadata': {
                            'file_path': str(file_path),
                            'chunk_index': chunk['index'],
                            'start_char': chunk['start_char'],
                            'processing_method': 'basic'
                        }
                    })
            except Exception as e:
                logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return chunks
    
    def similarity(self, 
                   text1: str, 
                   text2: str,
                   method: str = 'cosine') -> float:
        """
        ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            method: ìœ ì‚¬ë„ ê³„ì‚° ë°©ë²•
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜
        """
        emb1 = self.encode_single(text1)
        emb2 = self.encode_single(text2)
        
        if method == 'cosine':
            # Reshape for sklearn
            emb1_2d = emb1.reshape(1, -1)
            emb2_2d = emb2.reshape(1, -1)
            return cosine_similarity(emb1_2d, emb2_2d)[0][0]
        elif method == 'dot':
            return np.dot(emb1, emb2)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìœ ì‚¬ë„ ë°©ë²•: {method}")
    
    def find_most_similar(self, 
                         query: str, 
                         candidates: List[str],
                         top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ í›„ë³´ë“¤ì„ ì°¾ê¸°
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            candidates: í›„ë³´ í…ìŠ¤íŠ¸ë“¤
            top_k: ë°˜í™˜í•  ìƒìœ„ kê°œ
            
        Returns:
            ìœ ì‚¬ë„ ì •ë³´ê°€ í¬í•¨ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates:
            return []
        
        query_embedding = self.encode_single(query)
        candidate_embeddings = self.encode_batch(candidates)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        query_2d = query_embedding.reshape(1, -1)
        candidates_2d = np.array(candidate_embeddings)
        
        similarities = cosine_similarity(query_2d, candidates_2d)[0]
        
        # ê²°ê³¼ ì •ë ¬
        results = []
        for i, similarity in enumerate(similarities):
            results.append({
                'index': i,
                'text': candidates[i],
                'similarity': float(similarity)
            })
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return results[:top_k]
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            'model_type': 'TF-IDF',
            'embedding_dimension': self.embedding_dim,
            'korean_processor': 'Kiwi' if KIWI_AVAILABLE else 'Basic',
            'docling_available': True,
            'cache_enabled': True,
            'cache_dir': str(self.cache_dir),
            'corpus_size': len(self.corpus),
            'is_fitted': self.is_fitted
        }
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        try:
            import shutil
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(exist_ok=True)
            logger.info("âœ… ì„ë² ë”© ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# ì „ì—­ ì„ë² ë”© ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_embedding_service = None

def get_korean_embedding_service() -> KoreanEmbeddingService:
    """í•œêµ­ì–´ ì„ë² ë”© ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = KoreanEmbeddingService()
    return _embedding_service

def encode_korean_text(text: str) -> np.ndarray:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì„ë² ë”© í—¬í¼ í•¨ìˆ˜"""
    service = get_korean_embedding_service()
    return service.encode_single(text)

def encode_korean_texts(texts: List[str]) -> List[np.ndarray]:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© í—¬í¼ í•¨ìˆ˜"""
    service = get_korean_embedding_service()
    return service.encode_batch(texts)

def process_korean_document(file_path: str) -> List[Dict[str, Any]]:
    """í•œêµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜"""
    service = get_korean_embedding_service()
    return service.process_document_with_docling(file_path)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    service = KoreanEmbeddingService()
    
    # ìƒ˜í”Œ ì½”í¼ìŠ¤ë¡œ í•™ìŠµ
    sample_corpus = [
        "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ AIì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
        "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” ë„ì „ì ì¸ ê³¼ì œì…ë‹ˆë‹¤.",
        "ë°ì´í„° ë¶„ì„ì„ í†µí•´ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "ê³ ê° ì„œë¹„ìŠ¤ í’ˆì§ˆ í–¥ìƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
    ]
    
    service.fit_corpus(sample_corpus)
    
    test_texts = [
        "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "AIëŠ” ë§ì€ ì‚°ì—…ì„ ë³€í™”ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.",
        "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.",
        "ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ê³  ì‹¶ì–´ìš”."
    ]
    
    # ë°°ì¹˜ ì„ë² ë”© í…ŒìŠ¤íŠ¸
    embeddings = service.encode_batch(test_texts)
    print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
    print(f"   ì„ë² ë”© ì°¨ì›: {len(embeddings[0])}")
    
    # ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
    query = "AI ê¸°ìˆ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
    similar_results = service.find_most_similar(query, test_texts, top_k=3)
    
    print(f"\nğŸ” ì¿¼ë¦¬: {query}")
    print("ğŸ“Š ìœ ì‚¬í•œ í…ìŠ¤íŠ¸:")
    for result in similar_results:
        print(f"   - {result['text']} (ìœ ì‚¬ë„: {result['similarity']:.3f})")
    
    # ëª¨ë¸ ì •ë³´
    print(f"\nğŸ“Œ ëª¨ë¸ ì •ë³´: {service.get_model_info()}")